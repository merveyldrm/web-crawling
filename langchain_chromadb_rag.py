import os
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
import pandas as pd

# LangChain imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import CSVLoader, TextLoader
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain.llms import OpenAI, Ollama
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document
from langchain.prompts import PromptTemplate

# ChromaDB
import chromadb
from chromadb.config import Settings

# Local imports
from advanced_comment_analyzer import AdvancedCommentAnalyzer
from priority_analyzer import PriorityAnalyzer

class LangChainChromaRAG:
    def __init__(
        self,
        persist_directory: str = "./chroma_db",
        use_openai: bool = False,
        openai_api_key: str = None,
        model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    ):
        """
        GeliÅŸmiÅŸ RAG sistemi - LangChain + ChromaDB
        
        Args:
            persist_directory: ChromaDB'nin saklanacaÄŸÄ± dizin
            use_openai: OpenAI embeddings kullanÄ±lsÄ±n mÄ±
            openai_api_key: OpenAI API anahtarÄ±
            model_name: Embedding modeli adÄ±
        """
        self.persist_directory = persist_directory
        self.use_openai = use_openai
        
        # Dizin oluÅŸtur
        Path(persist_directory).mkdir(exist_ok=True)
        
        # Embeddings seÃ§imi
        if use_openai and openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key
            self.embeddings = OpenAIEmbeddings()
            print("âœ… OpenAI embeddings kullanÄ±lacak")
        else:
            self.embeddings = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={'device': 'cpu'}  # GPU varsa 'cuda' yapÄ±n
            )
            print(f"âœ… HuggingFace embeddings: {model_name}")
        
        # ChromaDB client
        self.chroma_client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Vector store
        self.vector_store = None
        self.retriever = None
        self.qa_chain = None
        
        # Text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        # Local analyzers
        self.comment_analyzer = AdvancedCommentAnalyzer()
        self.priority_analyzer = PriorityAnalyzer()
        
        # Memory for conversations
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        print("ğŸš€ LangChain + ChromaDB RAG sistemi baÅŸlatÄ±ldÄ±")
    
    def load_comments_to_vectorstore(
        self,
        csv_file: str = "trendyol_comments.csv",
        collection_name: str = "trendyol_comments"
    ) -> int:
        """
        YorumlarÄ± CSV'den yÃ¼kleyip ChromaDB'ye vektÃ¶rize ederek kaydet
        
        Returns:
            YÃ¼klenen dokÃ¼man sayÄ±sÄ±
        """
        print(f"ğŸ“Š Yorumlar yÃ¼kleniyor: {csv_file}")
        
        # CSV dosyasÄ±nÄ± oku
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
            print(f"ğŸ“ {len(df)} yorum bulundu")
        except Exception as e:
            print(f"âŒ CSV okuma hatasÄ±: {e}")
            return 0
        
        # Documents listesi oluÅŸtur
        documents = []
        
        for idx, row in df.iterrows():
            # Yorum analizi yap
            analysis = self.comment_analyzer.analyze_comment(str(row.get('comment', '')))
            priority = self.priority_analyzer.analyze_comment_priority(str(row.get('comment', '')))
            
            # Metadata oluÅŸtur
            metadata = {
                'source': 'trendyol_csv',
                'user': str(row.get('user', 'Unknown')),
                'date': str(row.get('date', '')),
                'comment_id': str(idx),
                'sentiment_category': analysis.get('category_analysis', {}).get('highest_category', 'unknown'),
                'sentiment_confidence': analysis.get('category_analysis', {}).get('highest_confidence', 0.0),
                'priority_score': priority.get('priority_score', 0),
                'urgent_category': priority.get('urgent_category', 'none'),
                'chunk_id': f"comment_{idx}"
            }
            
            # Document oluÅŸtur
            doc_content = f"""
KullanÄ±cÄ±: {row.get('user', 'Anonim')}
Tarih: {row.get('date', 'Bilinmiyor')}
Yorum: {row.get('comment', '')}

Analiz Ã–zeti:
- Kategori: {analysis.get('category_analysis', {}).get('highest_category', 'unknown')}
- GÃ¼ven: {analysis.get('category_analysis', {}).get('highest_confidence', 0.0):.2f}
- Ã–ncelik: {priority.get('priority_score', 0)}/100
- Acil Kategori: {priority.get('urgent_category', 'none')}
            """.strip()
            
            document = Document(
                page_content=doc_content,
                metadata=metadata
            )
            
            documents.append(document)
        
        # Text splitting (opsiyonel, yorumlar genelde kÄ±sa olduÄŸu iÃ§in)
        print("âœ‚ï¸ Metin bÃ¶lÃ¼mleniyor...")
        split_docs = self.text_splitter.split_documents(documents)
        print(f"ğŸ“„ {len(split_docs)} dokÃ¼man parÃ§asÄ± oluÅŸturuldu")
        
        # ChromaDB'ye kaydet
        print("ğŸ’¾ ChromaDB'ye kaydediliyor...")
        self.vector_store = Chroma.from_documents(
            documents=split_docs,
            embedding=self.embeddings,
            persist_directory=self.persist_directory,
            collection_name=collection_name
        )
        
        self.vector_store.persist()
        print(f"âœ… {len(split_docs)} dokÃ¼man ChromaDB'ye kaydedildi")
        
        return len(split_docs)
    
    def add_external_knowledge(
        self,
        knowledge_sources: Dict[str, Any],
        collection_name: str = "external_knowledge"
    ):
        """
        Harici bilgi kaynaklarÄ±nÄ± vector store'a ekle
        
        Args:
            knowledge_sources: Bilgi kaynaklarÄ± dict'i
        """
        print("ğŸŒ Harici bilgi kaynaklarÄ± ekleniyor...")
        
        documents = []
        
        for source_name, source_data in knowledge_sources.items():
            if isinstance(source_data, dict):
                content = json.dumps(source_data, ensure_ascii=False, indent=2)
                doc_type = "structured_data"
            elif isinstance(source_data, list):
                content = "\n".join(str(item) for item in source_data)
                doc_type = "list_data"
            else:
                content = str(source_data)
                doc_type = "text_data"
            
            metadata = {
                'source': 'external_knowledge',
                'source_name': source_name,
                'doc_type': doc_type,
                'created_at': datetime.now().isoformat(),
                'chunk_id': f"ext_{source_name}"
            }
            
            document = Document(
                page_content=f"Kaynak: {source_name}\n\nÄ°Ã§erik:\n{content}",
                metadata=metadata
            )
            
            documents.append(document)
        
        # Text splitting
        split_docs = self.text_splitter.split_documents(documents)
        
        # EÄŸer vector store yoksa oluÅŸtur
        if self.vector_store is None:
            self.vector_store = Chroma.from_documents(
                documents=split_docs,
                embedding=self.embeddings,
                persist_directory=self.persist_directory,
                collection_name=collection_name
            )
        else:
            # Mevcut vector store'a ekle
            self.vector_store.add_documents(split_docs)
        
        self.vector_store.persist()
        print(f"âœ… {len(split_docs)} harici bilgi kaynaÄŸÄ± eklendi")
    
    def setup_retrieval_chain(
        self,
        llm_type: str = "openai",  # "openai" or "ollama"
        model_name: str = "gpt-3.5-turbo",
        temperature: float = 0.1,
        k: int = 4  # Retrieve top-k documents
    ):
        """
        Retrieval QA chain'ini kur
        
        Args:
            llm_type: LLM tipi
            model_name: Model adÄ±
            temperature: YaratÄ±cÄ±lÄ±k seviyesi
            k: KaÃ§ dokÃ¼man Ã§ekilecek
        """
        print(f"ğŸ¤– {llm_type.upper()} LLM chain kuruluyor...")
        
        if self.vector_store is None:
            raise ValueError("âŒ Vector store henÃ¼z oluÅŸturulmamÄ±ÅŸ! Ã–nce load_comments_to_vectorstore() Ã§alÄ±ÅŸtÄ±rÄ±n.")
        
        # LLM seÃ§imi
        if llm_type == "openai":
            if not os.getenv("OPENAI_API_KEY"):
                raise ValueError("âŒ OPENAI_API_KEY environment variable gerekli!")
            llm = OpenAI(temperature=temperature, model_name=model_name)
        elif llm_type == "ollama":
            llm = Ollama(model=model_name, temperature=temperature)
        else:
            raise ValueError(f"âŒ Desteklenmeyen LLM tipi: {llm_type}")
        
        # Retriever kur
        self.retriever = self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": k}
        )
        
        # Turkish-specific prompt template
        prompt_template = """
Trendyol Ã¼rÃ¼n yorumlarÄ± ve ilgili bilgi kaynaklarÄ±nÄ± kullanarak soruyu yanÄ±tla.

BaÄŸlam Bilgiler:
{context}

Soru: {question}

YanÄ±t KurallarÄ±:
1. Sadece verilen baÄŸlam bilgilerini kullan
2. TÃ¼rkÃ§e ve anlaÅŸÄ±lÄ±r bir dille yanÄ±t ver
3. Spesifik Ã¶rnekler ve veri noktalarÄ± kullan
4. EÄŸer cevap baÄŸlamda yoksa "Bu sorunun cevabÄ± mevcut verilerde bulunmuyor" de
5. Ã–ncelik skorlarÄ± ve kategorileri dahil et
6. Pratik Ã§Ã¶zÃ¼m Ã¶nerileri sun

YanÄ±t:
"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # QA Chain oluÅŸtur
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=self.retriever,
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
        
        print("âœ… Retrieval QA chain hazÄ±r!")
    
    def query(self, question: str, show_sources: bool = True) -> Dict[str, Any]:
        """
        RAG sistemine soru sor
        
        Args:
            question: Soru
            show_sources: Kaynak dokÃ¼manlarÄ± gÃ¶ster
            
        Returns:
            YanÄ±t ve kaynak bilgileri
        """
        if self.qa_chain is None:
            raise ValueError("âŒ QA chain henÃ¼z kurulmamÄ±ÅŸ! setup_retrieval_chain() Ã§alÄ±ÅŸtÄ±rÄ±n.")
        
        print(f"ğŸ” Soru iÅŸleniyor: {question}")
        
        # Query Ã§alÄ±ÅŸtÄ±r
        result = self.qa_chain({"query": question})
        
        response = {
            "question": question,
            "answer": result["result"],
            "timestamp": datetime.now().isoformat()
        }
        
        if show_sources:
            sources = []
            for doc in result.get("source_documents", []):
                source_info = {
                    "content": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                    "metadata": doc.metadata,
                    "relevance_score": getattr(doc, "score", "N/A")
                }
                sources.append(source_info)
            
            response["sources"] = sources
            response["source_count"] = len(sources)
        
        return response
    
    def get_similar_comments(
        self,
        query: str,
        k: int = 5,
        filter_by: Optional[Dict] = None
    ) -> List[Dict]:
        """
        Benzer yorumlarÄ± bul
        
        Args:
            query: Arama sorgusu
            k: KaÃ§ yorum dÃ¶ndÃ¼rÃ¼lecek
            filter_by: Metadata filtresi (Ã¶rn: {"priority_score": {"$gte": 70}})
        """
        if self.vector_store is None:
            raise ValueError("âŒ Vector store henÃ¼z oluÅŸturulmamÄ±ÅŸ!")
        
        print(f"ğŸ” Benzer yorumlar aranÄ±yor: '{query}'")
        
        # Similarity search
        if filter_by:
            results = self.vector_store.similarity_search(
                query, 
                k=k,
                filter=filter_by
            )
        else:
            results = self.vector_store.similarity_search(query, k=k)
        
        similar_comments = []
        for doc in results:
            similar_comments.append({
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": getattr(doc, "score", "N/A")
            })
        
        print(f"ğŸ“Š {len(similar_comments)} benzer yorum bulundu")
        return similar_comments
    
    def analyze_trends(self, category: str = None) -> Dict[str, Any]:
        """
        Trend analizi yap
        
        Args:
            category: Spesifik kategori analizi
        """
        if self.vector_store is None:
            raise ValueError("âŒ Vector store henÃ¼z oluÅŸturulmamÄ±ÅŸ!")
        
        print(f"ğŸ“ˆ Trend analizi: {category or 'Genel'}")
        
        # TÃ¼m dokÃ¼manlarÄ± Ã§ek
        all_docs = self.vector_store._collection.get()
        
        # Metadata analizi
        categories = {}
        priority_scores = []
        urgent_categories = {}
        
        for metadata in all_docs.get("metadatas", []):
            # Kategori sayÄ±mÄ±
            cat = metadata.get("sentiment_category", "unknown")
            categories[cat] = categories.get(cat, 0) + 1
            
            # Ã–ncelik skorlarÄ±
            priority = metadata.get("priority_score", 0)
            if isinstance(priority, (int, float)):
                priority_scores.append(priority)
            
            # Acil kategoriler
            urgent = metadata.get("urgent_category", "none")
            urgent_categories[urgent] = urgent_categories.get(urgent, 0) + 1
        
        # Ä°statistikler
        total_comments = len(all_docs.get("metadatas", []))
        avg_priority = sum(priority_scores) / len(priority_scores) if priority_scores else 0
        high_priority_count = len([p for p in priority_scores if p >= 70])
        
        trends = {
            "total_comments": total_comments,
            "category_distribution": categories,
            "urgent_category_distribution": urgent_categories,
            "priority_stats": {
                "average_priority": round(avg_priority, 2),
                "high_priority_count": high_priority_count,
                "high_priority_percentage": round((high_priority_count / total_comments) * 100, 2) if total_comments > 0 else 0
            },
            "analysis_timestamp": datetime.now().isoformat()
        }
        
        return trends
    
    def reset_vectorstore(self):
        """Vector store'u sÄ±fÄ±rla"""
        import shutil
        if os.path.exists(self.persist_directory):
            shutil.rmtree(self.persist_directory)
            print("ğŸ—‘ï¸ Vector store sÄ±fÄ±rlandÄ±")
        
        self.vector_store = None
        self.retriever = None
        self.qa_chain = None


def main():
    """Demo usage"""
    print("ğŸš€ LangChain + ChromaDB RAG Demo")
    
    # RAG sistemi oluÅŸtur
    rag = LangChainChromaRAG(
        persist_directory="./chroma_trendyol_rag",
        use_openai=False,  # HuggingFace kullan (Ã¼cretsiz)
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )
    
    # Ã–rnek harici bilgi kaynaklarÄ±
    external_knowledge = {
        "kargo_cozumleri": [
            "Kargo gecikmelerine karÅŸÄ± alternatif teslimat seÃ§enekleri sunun",
            "Express kargo ile 1 gÃ¼n teslimat garantisi verin",
            "Kargo takip SMS/email sistemi kurun"
        ],
        "kalite_standartlari": {
            "tekstil": "KumaÅŸ kalitesi minimum %80 pamuk olmalÄ±",
            "elektronik": "CE sertifikasÄ± zorunlu",
            "kozmetik": "Dermatolojik test raporu gerekli"
        },
        "musteri_hizmetleri": {
            "yanit_suresi": "4 saat iÃ§inde",
            "cozum_orani": "%95",
            "diller": ["TÃ¼rkÃ§e", "Ä°ngilizce"]
        }
    }
    
    # YorumlarÄ± yÃ¼kle
    rag.load_comments_to_vectorstore("trendyol_comments.csv")
    
    # Harici bilgileri ekle  
    rag.add_external_knowledge(external_knowledge)
    
    # LLM chain kur (Ollama kullanarak)
    try:
        rag.setup_retrieval_chain(
            llm_type="ollama",  # Ãœcretsiz local LLM
            model_name="llama2:7b",  # veya "mistral"
            temperature=0.1
        )
        
        # Ã–rnek sorgular
        questions = [
            "Kargo sorunlarÄ± hakkÄ±nda ne tÃ¼r ÅŸikayetler var?",
            "En yÃ¼ksek Ã¶ncelikli problemler neler?",
            "Kalite konusunda hangi Ã¼rÃ¼nlerde sorun yaÅŸanÄ±yor?",
            "MÃ¼ÅŸteri memnuniyetini artÄ±rmak iÃ§in ne Ã¶nerirsin?"
        ]
        
        for question in questions:
            print(f"\n{'='*50}")
            print(f"Soru: {question}")
            print('='*50)
            
            result = rag.query(question)
            print(f"YanÄ±t: {result['answer']}")
            print(f"Kaynak sayÄ±sÄ±: {result.get('source_count', 0)}")
            
    except Exception as e:
        print(f"âš ï¸ LLM setup hatasÄ±: {e}")
        print("ğŸ’¡ Ollama kurulumu: https://ollama.ai/download")
        print("ğŸ’¡ Model indirme: ollama pull llama2:7b")
    
    # Trend analizi
    print(f"\n{'='*50}")
    print("ğŸ“ˆ TREND ANALÄ°ZÄ°")
    print('='*50)
    
    trends = rag.analyze_trends()
    print(json.dumps(trends, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main() 