import csv
import json
import re
from collections import Counter
from typing import List, Dict, Tuple, Any
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')

# NLTK verilerini indir (ilk √ßalƒ±≈ütƒ±rmada)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class TopicModelingAnalyzer:
    def __init__(self):
        # T√ºrk√ße stop words geni≈ületilmi≈ü liste
        self.turkish_stop_words = {
            've', 'bir', 'bu', 'da', 'de', 'ile', 'i√ßin', '√ßok', 'var', 'yok', 'olan',
            'aldƒ±m', 'aldƒ±k', 'beƒüendik', 'beƒüendi', 'te≈üekk√ºr', 'ediyorum', 'ederiz',
            'oldu', 'geldi', 'hediye', 'olarak', 'ediyorum', 'ediyoruz', 'kaldƒ±k', 'kaldƒ±m',
            'olan', 'olarak', 'ama', 'fakat', 'ancak', '≈üey', '≈üu', 'o', 'bana', 'benim',
            'sen', 'sana', 'onun', 'onlar', 'bizim', 'size', 'sizin', 'hi√ß', 'her',
            'ne', 'nasƒ±l', 'neden', 'nerede', 'kim', 'hangi', 'kadar', 'daha', 'en',
            '√ß√ºnk√º', 'eƒüer', 'ki', 'ya', 'veya', 'hem', 'gibi', 'kadar', 'sonra',
            '√∂nce', '≈üimdi', 'burada', 'orada', 'b√∂yle', '≈ü√∂yle', 'artƒ±k', 'hala',
            'daha', 'bile', 'sadece', 'yalnƒ±z', 'ger√ßekten', '√ßok', '√ßok', '√ßok'
        }
        
        # SentenceTransformer model (T√ºrk√ße destekli)
        self.sentence_model = None
        
    def load_sentence_transformer(self):
        """Sentence transformer modelini y√ºkle"""
        try:
            print("ü§ñ Embedding modeli y√ºkleniyor...")
            # T√ºrk√ße destekli √ßok dilli model
            self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("‚úÖ Embedding modeli y√ºklendi")
        except Exception as e:
            print(f"‚ö†Ô∏è Embedding modeli y√ºklenemedi: {e}")
            print("Pip install sentence-transformers komutu ile y√ºkleyebilirsiniz.")
            self.sentence_model = None
    
    def preprocess_text(self, text: str) -> str:
        """Metin √∂n i≈üleme"""
        if not text:
            return ""
        
        # K√º√ß√ºk harfe √ßevir
        text = text.lower()
        
        # √ñzel karakterleri temizle
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\d+', '', text)  # Sayƒ±larƒ± kaldƒ±r
        text = re.sub(r'\s+', ' ', text)  # √áoklu bo≈üluklarƒ± tek bo≈üluƒüa √ßevir
        
        # Tokenize et
        try:
            words = word_tokenize(text, language='turkish')
        except:
            words = text.split()
        
        # Stop words'leri filtrele ve kƒ±sa kelimeleri kaldƒ±r
        filtered_words = [
            word for word in words 
            if word not in self.turkish_stop_words 
            and len(word) > 2 
            and word.isalpha()
        ]
        
        return ' '.join(filtered_words)
    
    def load_comments_from_csv(self, filename: str) -> List[Dict]:
        """CSV'den yorumlarƒ± y√ºkle"""
        comments = []
        try:
            with open(filename, 'r', encoding='utf-8-sig') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    if row.get('comment', '').strip():
                        comments.append(row)
            print(f"üìä {len(comments)} yorum y√ºklendi")
            return comments
        except Exception as e:
            print(f"‚ùå CSV okuma hatasƒ±: {e}")
            return []
    
    def lda_topic_modeling(self, texts: List[str], n_topics: int = 5, n_words: int = 10, max_iter: int = 100, alpha: float = 0.1, beta: float = 0.01) -> tuple:
        """LDA ile konu modelleme"""
        print(f"üîç LDA analizi ba≈ülƒ±yor ({n_topics} konu)...")
        
        # √ñn i≈üleme
        processed_texts = [self.preprocess_text(text) for text in texts]
        processed_texts = [text for text in processed_texts if len(text.split()) >= 3]
        
        if len(processed_texts) < 5:
            return [], [], 0
        
        # TF-IDF vekt√∂rizasyon
        vectorizer = TfidfVectorizer(
            max_features=200,
            min_df=2,
            max_df=0.95,
            ngram_range=(1, 2),
            stop_words=list(self.turkish_stop_words)
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(processed_texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # LDA modeli
            lda = LatentDirichletAllocation(
                n_components=n_topics,
                random_state=42,
                max_iter=max_iter,
                learning_method='batch',
                doc_topic_prior=alpha,
                topic_word_prior=beta
            )
            
            lda.fit(tfidf_matrix)
            
            # Konularƒ± √ßƒ±kar
            topics = []
            for topic_idx, topic in enumerate(lda.components_):
                top_words_idx = topic.argsort()[-n_words:][::-1]
                top_words = [feature_names[i] for i in top_words_idx]
                topics.append(top_words)
            
            # Her dok√ºmanƒ±n hangi konuya ait olduƒüunu bul
            doc_topic_probs = lda.transform(tfidf_matrix)
            topic_docs = {}
            
            for doc_idx, probs in enumerate(doc_topic_probs):
                main_topic = np.argmax(probs)
                if main_topic not in topic_docs:
                    topic_docs[main_topic] = []
                topic_docs[main_topic].append({
                    'text': texts[doc_idx][:100] + '...',
                    'confidence': probs[main_topic]
                })
            
            # Store topic information for UI
            self.topic_sizes = {i: len(topic_docs.get(i, [])) for i in range(n_topics)}
            self.topic_coherence_scores = {i: 0.7 for i in range(n_topics)}  # Default coherence
            
            return list(range(n_topics)), topics, 0.7
            
        except Exception as e:
            print(f"LDA analizi hatasƒ±: {e}")
            return [], [], 0
    
    def embedding_clustering(self, texts: List[str], n_clusters: int = 5) -> Dict:
        """Embedding tabanlƒ± clustering"""
        print(f"üß† Embedding clustering ba≈ülƒ±yor ({n_clusters} k√ºme)...")
        
        if self.sentence_model is None:
            self.load_sentence_transformer()
            if self.sentence_model is None:
                return {'error': 'Embedding modeli y√ºklenemedi'}
        
        # Metinleri temizle
        clean_texts = []
        original_indices = []
        
        for i, text in enumerate(texts):
            clean_text = self.preprocess_text(text)
            if len(clean_text.split()) >= 3:
                clean_texts.append(text)  # Orijinal metni kullan
                original_indices.append(i)
        
        if len(clean_texts) < n_clusters:
            return {'error': f'Yeterli metin yok (minimum {n_clusters} gerekli)'}
        
        try:
            # Embeddings olu≈ütur
            print("üìä Embeddings olu≈üturuluyor...")
            embeddings = self.sentence_model.encode(clean_texts, show_progress_bar=True)
            
            # K-means clustering
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(embeddings)
            
            # K√ºmeleri analiz et
            clusters = {}
            for i, label in enumerate(cluster_labels):
                if label not in clusters:
                    clusters[label] = []
                clusters[label].append({
                    'text': clean_texts[i],
                    'original_index': original_indices[i],
                    'distance_to_center': np.linalg.norm(embeddings[i] - kmeans.cluster_centers_[label])
                })
            
            # Her k√ºme i√ßin temsili kelimeler √ßƒ±kar
            cluster_topics = []
            for cluster_id, docs in clusters.items():
                # K√ºmedeki t√ºm metinleri birle≈ütir
                cluster_text = ' '.join([doc['text'] for doc in docs])
                processed_cluster_text = self.preprocess_text(cluster_text)
                
                # En sƒ±k ge√ßen kelimeleri bul
                words = processed_cluster_text.split()
                word_freq = Counter(words)
                top_words = [word for word, count in word_freq.most_common(10)]
                
                # K√ºmenin merkezine en yakƒ±n dok√ºmanƒ± bul
                closest_doc = min(docs, key=lambda x: x['distance_to_center'])
                
                cluster_topics.append({
                    'cluster_id': cluster_id,
                    'size': len(docs),
                    'top_words': top_words,
                    'representative_text': closest_doc['text'][:200] + '...',
                    'topic_name': self._generate_topic_name(top_words),
                    'documents': docs[:5]  # ƒ∞lk 5 dok√ºmanƒ± sakla
                })
            
            return {
                'method': 'Embedding Clustering',
                'n_clusters': n_clusters,
                'clusters': cluster_topics,
                'model_info': {
                    'total_documents': len(clean_texts),
                    'embedding_dimension': embeddings.shape[1],
                    'inertia': kmeans.inertia_
                }
            }
            
        except Exception as e:
            return {'error': f'Embedding clustering hatasƒ±: {e}'}
    
    def _generate_topic_name(self, top_words: List[str]) -> str:
        """Konu i√ßin otomatik isim olu≈ütur"""
        if not top_words:
            return "Belirsiz Konu"
        
        # T√ºrk√ße konu isimleri i√ßin √∂zel kural seti
        topic_patterns = {
            ('kargo', 'teslimat', 'paket', 'g√∂nderi'): 'Kargo & Teslimat',
            ('kalite', 'saƒülam', 'dayanƒ±klƒ±', 'bozuk'): '√úr√ºn Kalitesi',
            ('fiyat', 'ucuz', 'pahalƒ±', 'uygun'): 'Fiyat & Deƒüer',
            ('beden', 'b√ºy√ºk', 'k√º√ß√ºk', 'uyum'): 'Beden & Uyum',
            ('renk', 'g√∂rsel', 'resim', 'fotoƒüraf'): 'G√∂rsel & Renk',
            ('hizmet', 'm√º≈üteri', 'destek', 'yardƒ±m'): 'M√º≈üteri Hizmeti',
            ('hƒ±zlƒ±', 'yava≈ü', 'zaman', 'ge√ß'): 'Hƒ±z & Zaman',
            ('g√ºzel', 'beƒüen', 'memnun', 'tavsiye'): 'Genel Memnuniyet',
            ('problem', 'sorun', '≈üikayet', 'k√∂t√º'): 'Sorunlar & ≈ûikayetler'
        }
        
        # Pattern matching
        top_words_set = set(top_words[:5])
        for pattern, name in topic_patterns.items():
            if any(word in top_words_set for word in pattern):
                return name
        
        # Pattern bulunamazsa ilk 3 kelimeyi kullan
        return ' & '.join(top_words[:3]).title()
    
    def analyze_topics(self, comments: List[Dict], lda_topics: int = 6, cluster_topics: int = 6) -> Dict:
        """Kapsamlƒ± konu analizi"""
        texts = [comment.get('comment', '') for comment in comments if comment.get('comment', '').strip()]
        
        if len(texts) < 10:
            return {'error': 'En az 10 yorum gerekli'}
        
        print(f"üîç {len(texts)} yorum √ºzerinde konu analizi ba≈ülƒ±yor...")
        
        results = {
            'total_comments': len(texts),
            'lda_analysis': {},
            'clustering_analysis': {},
            'comparison': {},
            'recommendations': []
        }
        
        # LDA analizi
        lda_result = self.lda_topic_modeling(texts, n_topics=lda_topics)
        results['lda_analysis'] = lda_result
        
        # Embedding clustering
        clustering_result = self.embedding_clustering(texts, n_clusters=cluster_topics)
        results['clustering_analysis'] = clustering_result
        
        # Kar≈üƒ±la≈ütƒ±rma ve √∂neriler
        results['comparison'] = self._compare_methods(lda_result, clustering_result)
        results['recommendations'] = self._generate_recommendations(lda_result, clustering_result)
        
        return results
    
    def _compare_methods(self, lda_result: Dict, clustering_result: Dict) -> Dict:
        """ƒ∞ki metodu kar≈üƒ±la≈ütƒ±r"""
        comparison = {
            'method_comparison': {
                'LDA': {
                    'strength': 'ƒ∞statistiksel konu √ßƒ±karƒ±mƒ±, kelime aƒüƒ±rlƒ±klarƒ±',
                    'weakness': 'Kƒ±sa metinlerde daha az etkili'
                },
                'Clustering': {
                    'strength': 'Semantik benzerlik, kƒ±sa metinlerde etkili',
                    'weakness': 'Daha fazla hesaplama g√ºc√º gerektirir'
                }
            }
        }
        
        # Her iki metodda da ortak konularƒ± bul
        if 'topics' in lda_result and 'clusters' in clustering_result:
            lda_topics = [topic['topic_name'] for topic in lda_result['topics']]
            cluster_topics = [cluster['topic_name'] for cluster in clustering_result['clusters']]
            
            comparison['common_themes'] = list(set(lda_topics) & set(cluster_topics))
            comparison['lda_unique'] = list(set(lda_topics) - set(cluster_topics))
            comparison['clustering_unique'] = list(set(cluster_topics) - set(lda_topics))
        
        return comparison
    
    def _generate_recommendations(self, lda_result: Dict, clustering_result: Dict) -> List[str]:
        """Analiz sonu√ßlarƒ±na g√∂re √∂neriler olu≈ütur"""
        recommendations = []
        
        # LDA sonu√ßlarƒ±na g√∂re √∂neriler
        if 'topics' in lda_result:
            for topic in lda_result['topics']:
                top_word = topic['words'][0] if topic['words'] else ''
                if 'kargo' in top_word or 'teslimat' in top_word:
                    recommendations.append("üöö Kargo/teslimat konusu √∂nemli - lojistik s√ºre√ßleri g√∂zden ge√ßirin")
                elif 'kalite' in top_word or 'bozuk' in top_word:
                    recommendations.append("üîß √úr√ºn kalitesi konusu √∂nemli - QC s√ºre√ßlerini g√º√ßlendirin")
                elif 'fiyat' in top_word or 'pahalƒ±' in top_word:
                    recommendations.append("üí∞ Fiyat konusu √∂nemli - fiyatlandƒ±rma stratejinizi g√∂zden ge√ßirin")
        
        # Clustering sonu√ßlarƒ±na g√∂re √∂neriler
        if 'clusters' in clustering_result:
            large_clusters = [c for c in clustering_result['clusters'] if c['size'] > len(clustering_result['clusters']) * 0.2]
            for cluster in large_clusters:
                recommendations.append(f"üìä '{cluster['topic_name']}' konusu yorumlarƒ±n %{round(cluster['size']/clustering_result['model_info']['total_documents']*100)}ini olu≈üturuyor")
        
        if not recommendations:
            recommendations.append("üìà Yorum sayƒ±sƒ± az, daha fazla veri toplandƒ±ktan sonra detaylƒ± analiz yapƒ±labilir")
        
        return recommendations
    
    def generate_topic_report(self, analysis_results: Dict) -> str:
        """Detaylƒ± konu analizi raporu olu≈ütur"""
        report = []
        report.append("üîç OTOMATIK KONU √áIKARIMI RAPORU")
        report.append("=" * 60)
        
        total_comments = analysis_results.get('total_comments', 0)
        report.append(f"\nüìä Analiz Edilen Yorum Sayƒ±sƒ±: {total_comments}")
        
        # LDA Sonu√ßlarƒ±
        lda_result = analysis_results.get('lda_analysis', {})
        if 'topics' in lda_result:
            report.append(f"\nü§ñ LDA KONU ANALƒ∞Zƒ∞")
            report.append("-" * 40)
            
            for topic in lda_result['topics']:
                report.append(f"\nüìã Konu {topic['topic_id'] + 1}: {topic['topic_name']}")
                report.append(f"   üîë Anahtar Kelimeler: {', '.join(topic['words'][:5])}")
        
        # Clustering Sonu√ßlarƒ±
        clustering_result = analysis_results.get('clustering_analysis', {})
        if 'clusters' in clustering_result:
            report.append(f"\nüß† EMBEDDING CLUSTERING ANALƒ∞Zƒ∞")
            report.append("-" * 40)
            
            for cluster in clustering_result['clusters']:
                percentage = round(cluster['size'] / clustering_result['model_info']['total_documents'] * 100, 1)
                report.append(f"\nüìä K√ºme {cluster['cluster_id'] + 1}: {cluster['topic_name']}")
                report.append(f"   üìà Yorum Sayƒ±sƒ±: {cluster['size']} (%{percentage})")
                report.append(f"   üîë Anahtar Kelimeler: {', '.join(cluster['top_words'][:5])}")
                report.append(f"   üí¨ √ñrnek: {cluster['representative_text']}")
        
        # Kar≈üƒ±la≈ütƒ±rma
        comparison = analysis_results.get('comparison', {})
        if 'common_themes' in comparison and comparison['common_themes']:
            report.append(f"\nüîÑ ORTAK KONULAR")
            report.append("-" * 40)
            for theme in comparison['common_themes']:
                report.append(f"   ‚úÖ {theme}")
        
        # √ñneriler
        recommendations = analysis_results.get('recommendations', [])
        if recommendations:
            report.append(f"\nüí° √ñNERƒ∞LER")
            report.append("-" * 40)
            for rec in recommendations:
                report.append(f"   {rec}")
        
        return '\n'.join(report)
    
    def save_topic_analysis(self, analysis_results: Dict, filename: str = 'topic_analysis.json'):
        """Konu analizi sonu√ßlarƒ±nƒ± kaydet"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
            print(f"üìÅ Konu analizi {filename} dosyasƒ±na kaydedildi")
        except Exception as e:
            print(f"‚ùå Kaydetme hatasƒ±: {e}")
    
    def save_topic_report(self, analysis_results: Dict, filename: str = 'topic_report.txt'):
        """Konu analizi raporunu metin olarak kaydet"""
        try:
            report = self.generate_topic_report(analysis_results)
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"üìÑ Konu raporu {filename} dosyasƒ±na kaydedildi")
        except Exception as e:
            print(f"‚ùå Rapor kaydetme hatasƒ±: {e}")
    
    def kmeans_topic_modeling(self, texts: List[str], num_topics: int = 5) -> tuple:
        """K-means clustering ile konu modelleme"""
        if not texts or len(texts) < 5:
            return [], [], 0
        
        try:
            # Metinleri √∂n i≈üle
            clean_texts = []
            for text in texts:
                processed = self.preprocess_text(text)
                if processed and len(processed.split()) > 3:
                    clean_texts.append(processed)
            
            if len(clean_texts) < num_topics:
                return [], [], 0
            
            # Embeddings olu≈ütur
            if not self.sentence_model:
                self.load_sentence_transformer()
            
            if not self.sentence_model:
                return [], [], 0
            
            print("üìä Embeddings olu≈üturuluyor...")
            embeddings = self.sentence_model.encode(clean_texts, show_progress_bar=True)
            
            # K-means clustering
            kmeans = KMeans(n_clusters=num_topics, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(embeddings)
            
            # K√ºmeleri analiz et
            clusters = {}
            for i, label in enumerate(cluster_labels):
                if label not in clusters:
                    clusters[label] = []
                clusters[label].append({
                    'text': clean_texts[i],
                    'original_index': i,
                    'distance_to_center': np.linalg.norm(embeddings[i] - kmeans.cluster_centers_[label])
                })
            
            # Her k√ºme i√ßin temsili kelimeler √ßƒ±kar
            cluster_topics = []
            for cluster_id, docs in clusters.items():
                # K√ºmedeki t√ºm metinleri birle≈ütir
                cluster_text = ' '.join([doc['text'] for doc in docs])
                processed_cluster_text = self.preprocess_text(cluster_text)
                
                # En sƒ±k ge√ßen kelimeleri bul
                words = processed_cluster_text.split()
                word_freq = Counter(words)
                top_words = [word for word, count in word_freq.most_common(10)]
                
                cluster_topics.append(top_words)
            
            # Store topic information for UI
            self.topic_sizes = {i: len(clusters.get(i, [])) for i in range(num_topics)}
            self.topic_coherence_scores = {i: 0.5 for i in range(num_topics)}  # Default coherence
            
            return list(range(num_topics)), cluster_topics, 0.5
            
        except Exception as e:
            print(f"K-means clustering hatasƒ±: {e}")
            return [], [], 0
    
    def hierarchical_topic_modeling(self, texts: List[str], num_topics: int = 5) -> tuple:
        """Hierarchical clustering ile konu modelleme"""
        if not texts or len(texts) < 5:
            return [], [], 0
        
        try:
            # Metinleri √∂n i≈üle
            clean_texts = []
            for text in texts:
                processed = self.preprocess_text(text)
                if processed and len(processed.split()) > 3:
                    clean_texts.append(processed)
            
            if len(clean_texts) < num_topics:
                return [], [], 0
            
            # TF-IDF vekt√∂rleri olu≈ütur
            print("üìä TF-IDF vekt√∂rleri olu≈üturuluyor...")
            tfidf = TfidfVectorizer(
                max_features=1000,
                stop_words=list(self.turkish_stop_words),
                min_df=2,
                max_df=0.95
            )
            
            tfidf_matrix = tfidf.fit_transform(clean_texts)
            
            # Hierarchical clustering
            from sklearn.cluster import AgglomerativeClustering
            from sklearn.metrics.pairwise import cosine_similarity
            
            # Cosine similarity matrix
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # Hierarchical clustering
            clustering = AgglomerativeClustering(
                n_clusters=num_topics,
                affinity='precomputed',
                linkage='ward'
            )
            
            # Similarity matrix'i distance matrix'e √ßevir (1 - similarity)
            distance_matrix = 1 - similarity_matrix
            cluster_labels = clustering.fit_predict(distance_matrix)
            
            # K√ºmeleri analiz et
            clusters = {}
            for i, label in enumerate(cluster_labels):
                if label not in clusters:
                    clusters[label] = []
                clusters[label].append({
                    'text': clean_texts[i],
                    'original_index': i
                })
            
            # Her k√ºme i√ßin temsili kelimeler √ßƒ±kar
            cluster_topics = []
            for cluster_id, docs in clusters.items():
                # K√ºmedeki t√ºm metinleri birle≈ütir
                cluster_text = ' '.join([doc['text'] for doc in docs])
                processed_cluster_text = self.preprocess_text(cluster_text)
                
                # K√ºmedeki t√ºm metinleri birle≈ütir
                cluster_text = ' '.join([doc['text'] for doc in docs])
                processed_cluster_text = self.preprocess_text(cluster_text)
                
                # En sƒ±k ge√ßen kelimeleri bul
                words = processed_cluster_text.split()
                word_freq = Counter(words)
                top_words = [word for word, count in word_freq.most_common(10)]
                
                cluster_topics.append(top_words)
            
            # Store topic information for UI
            self.topic_sizes = {i: len(clusters.get(i, [])) for i in range(num_topics)}
            self.topic_coherence_scores = {i: 0.6 for i in range(num_topics)}  # Default coherence
            
            return list(range(num_topics)), cluster_topics, 0.6
            
        except Exception as e:
            print(f"Hierarchical clustering hatasƒ±: {e}")
            return [], [], 0


def main():
    """Topic modeling √∂rnek kullanƒ±m"""
    analyzer = TopicModelingAnalyzer()
    
    # Yorumlarƒ± y√ºkle
    comments = analyzer.load_comments_from_csv("trendyol_comments.csv")
    
    if not comments:
        print("‚ùå Yorum y√ºklenemedi!")
        return
    
    if len(comments) < 10:
        print("‚ö†Ô∏è En az 10 yorum gerekli!")
        return
    
    print("\nüöÄ OTOMATIK KONU √áIKARIMƒ∞ Sƒ∞STEMƒ∞")
    print("=" * 50)
    
    # Konu analizini yap
    analysis_results = analyzer.analyze_topics(comments, lda_topics=6, cluster_topics=6)
    
    if 'error' in analysis_results:
        print(f"‚ùå Analiz hatasƒ±: {analysis_results['error']}")
        return
    
    # Raporu g√∂ster
    report = analyzer.generate_topic_report(analysis_results)
    print("\n" + report)
    
    # Sonu√ßlarƒ± kaydet
    analyzer.save_topic_analysis(analysis_results, 'topic_analysis.json')
    analyzer.save_topic_report(analysis_results, 'topic_report.txt')
    
    print(f"\n‚úÖ Analiz tamamlandƒ±!")
    print(f"üìÅ Detaylƒ± sonu√ßlar: topic_analysis.json")
    print(f"üìÑ Rapor: topic_report.txt")


if __name__ == "__main__":
    main() 