import csv
import json
import re
from collections import Counter
from typing import List, Dict, Tuple, Any
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')

# NLTK verilerini indir (ilk Ã§alÄ±ÅŸtÄ±rmada)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class TopicModelingAnalyzer:
    def __init__(self):
        # TÃ¼rkÃ§e stop words geniÅŸletilmiÅŸ liste
        self.turkish_stop_words = {
            've', 'bir', 'bu', 'da', 'de', 'ile', 'iÃ§in', 'Ã§ok', 'var', 'yok', 'olan',
            'aldÄ±m', 'aldÄ±k', 'beÄŸendik', 'beÄŸendi', 'teÅŸekkÃ¼r', 'ediyorum', 'ederiz',
            'oldu', 'geldi', 'hediye', 'olarak', 'ediyorum', 'ediyoruz', 'kaldÄ±k', 'kaldÄ±m',
            'olan', 'olarak', 'ama', 'fakat', 'ancak', 'ÅŸey', 'ÅŸu', 'o', 'bana', 'benim',
            'sen', 'sana', 'onun', 'onlar', 'bizim', 'size', 'sizin', 'hiÃ§', 'her',
            'ne', 'nasÄ±l', 'neden', 'nerede', 'kim', 'hangi', 'kadar', 'daha', 'en',
            'Ã§Ã¼nkÃ¼', 'eÄŸer', 'ki', 'ya', 'veya', 'hem', 'gibi', 'kadar', 'sonra',
            'Ã¶nce', 'ÅŸimdi', 'burada', 'orada', 'bÃ¶yle', 'ÅŸÃ¶yle', 'artÄ±k', 'hala',
            'daha', 'bile', 'sadece', 'yalnÄ±z', 'gerÃ§ekten', 'Ã§ok', 'Ã§ok', 'Ã§ok'
        }
        
        # SentenceTransformer model (TÃ¼rkÃ§e destekli)
        self.sentence_model = None
        
    def load_sentence_transformer(self):
        """Sentence transformer modelini yÃ¼kle"""
        try:
            print("ğŸ¤– Embedding modeli yÃ¼kleniyor...")
            # TÃ¼rkÃ§e destekli Ã§ok dilli model
            self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("âœ… Embedding modeli yÃ¼klendi")
        except Exception as e:
            print(f"âš ï¸ Embedding modeli yÃ¼klenemedi: {e}")
            print("Pip install sentence-transformers komutu ile yÃ¼kleyebilirsiniz.")
            self.sentence_model = None
    
    def preprocess_text(self, text: str) -> str:
        """Metin Ã¶n iÅŸleme"""
        if not text:
            return ""
        
        # KÃ¼Ã§Ã¼k harfe Ã§evir
        text = text.lower()
        
        # Ã–zel karakterleri temizle
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\d+', '', text)  # SayÄ±larÄ± kaldÄ±r
        text = re.sub(r'\s+', ' ', text)  # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
        
        # Tokenize et
        try:
            words = word_tokenize(text, language='turkish')
        except:
            words = text.split()
        
        # Stop words'leri filtrele ve kÄ±sa kelimeleri kaldÄ±r
        filtered_words = [
            word for word in words 
            if word not in self.turkish_stop_words 
            and len(word) > 2 
            and word.isalpha()
        ]
        
        return ' '.join(filtered_words)
    
    def load_comments_from_csv(self, filename: str) -> List[Dict]:
        """CSV'den yorumlarÄ± yÃ¼kle"""
        comments = []
        try:
            with open(filename, 'r', encoding='utf-8-sig') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    if row.get('comment', '').strip():
                        comments.append(row)
            print(f"ğŸ“Š {len(comments)} yorum yÃ¼klendi")
            return comments
        except Exception as e:
            print(f"âŒ CSV okuma hatasÄ±: {e}")
            return []
    
    def lda_topic_modeling(self, texts: List[str], n_topics: int = 5, n_words: int = 10) -> Dict:
        """LDA ile konu modelleme"""
        print(f"ğŸ” LDA analizi baÅŸlÄ±yor ({n_topics} konu)...")
        
        # Ã–n iÅŸleme
        processed_texts = [self.preprocess_text(text) for text in texts]
        processed_texts = [text for text in processed_texts if len(text.split()) >= 3]
        
        if len(processed_texts) < 5:
            return {'error': 'Yeterli metin bulunamadÄ±'}
        
        # TF-IDF vektÃ¶rizasyon
        vectorizer = TfidfVectorizer(
            max_features=200,
            min_df=2,
            max_df=0.95,
            ngram_range=(1, 2),
            stop_words=list(self.turkish_stop_words)
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(processed_texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # LDA modeli
            lda = LatentDirichletAllocation(
                n_components=n_topics,
                random_state=42,
                max_iter=50,
                learning_method='online'
            )
            
            lda.fit(tfidf_matrix)
            
            # KonularÄ± Ã§Ä±kar
            topics = []
            for topic_idx, topic in enumerate(lda.components_):
                top_words_idx = topic.argsort()[-n_words:][::-1]
                top_words = [feature_names[i] for i in top_words_idx]
                top_weights = [topic[i] for i in top_words_idx]
                
                topics.append({
                    'topic_id': topic_idx,
                    'words': top_words,
                    'weights': top_weights,
                    'topic_name': self._generate_topic_name(top_words)
                })
            
            # Her dokÃ¼manÄ±n hangi konuya ait olduÄŸunu bul
            doc_topic_probs = lda.transform(tfidf_matrix)
            document_topics = []
            
            for doc_idx, probs in enumerate(doc_topic_probs):
                main_topic = np.argmax(probs)
                confidence = probs[main_topic]
                
                document_topics.append({
                    'document_index': doc_idx,
                    'original_text': texts[doc_idx][:100] + '...',
                    'main_topic': main_topic,
                    'confidence': confidence,
                    'all_probabilities': probs.tolist()
                })
            
            return {
                'method': 'LDA',
                'n_topics': n_topics,
                'topics': topics,
                'document_topics': document_topics,
                'model_info': {
                    'total_documents': len(processed_texts),
                    'vocabulary_size': len(feature_names),
                    'perplexity': lda.perplexity(tfidf_matrix)
                }
            }
            
        except Exception as e:
            return {'error': f'LDA analizi hatasÄ±: {e}'}
    
    def embedding_clustering(self, texts: List[str], n_clusters: int = 5) -> Dict:
        """Embedding tabanlÄ± clustering"""
        print(f"ğŸ§  Embedding clustering baÅŸlÄ±yor ({n_clusters} kÃ¼me)...")
        
        if self.sentence_model is None:
            self.load_sentence_transformer()
            if self.sentence_model is None:
                return {'error': 'Embedding modeli yÃ¼klenemedi'}
        
        # Metinleri temizle
        clean_texts = []
        original_indices = []
        
        for i, text in enumerate(texts):
            clean_text = self.preprocess_text(text)
            if len(clean_text.split()) >= 3:
                clean_texts.append(text)  # Orijinal metni kullan
                original_indices.append(i)
        
        if len(clean_texts) < n_clusters:
            return {'error': f'Yeterli metin yok (minimum {n_clusters} gerekli)'}
        
        try:
            # Embeddings oluÅŸtur
            print("ğŸ“Š Embeddings oluÅŸturuluyor...")
            embeddings = self.sentence_model.encode(clean_texts, show_progress_bar=True)
            
            # K-means clustering
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(embeddings)
            
            # KÃ¼meleri analiz et
            clusters = {}
            for i, label in enumerate(cluster_labels):
                if label not in clusters:
                    clusters[label] = []
                clusters[label].append({
                    'text': clean_texts[i],
                    'original_index': original_indices[i],
                    'distance_to_center': np.linalg.norm(embeddings[i] - kmeans.cluster_centers_[label])
                })
            
            # Her kÃ¼me iÃ§in temsili kelimeler Ã§Ä±kar
            cluster_topics = []
            for cluster_id, docs in clusters.items():
                # KÃ¼medeki tÃ¼m metinleri birleÅŸtir
                cluster_text = ' '.join([doc['text'] for doc in docs])
                processed_cluster_text = self.preprocess_text(cluster_text)
                
                # En sÄ±k geÃ§en kelimeleri bul
                words = processed_cluster_text.split()
                word_freq = Counter(words)
                top_words = [word for word, count in word_freq.most_common(10)]
                
                # KÃ¼menin merkezine en yakÄ±n dokÃ¼manÄ± bul
                closest_doc = min(docs, key=lambda x: x['distance_to_center'])
                
                cluster_topics.append({
                    'cluster_id': cluster_id,
                    'size': len(docs),
                    'top_words': top_words,
                    'representative_text': closest_doc['text'][:200] + '...',
                    'topic_name': self._generate_topic_name(top_words),
                    'documents': docs[:5]  # Ä°lk 5 dokÃ¼manÄ± sakla
                })
            
            return {
                'method': 'Embedding Clustering',
                'n_clusters': n_clusters,
                'clusters': cluster_topics,
                'model_info': {
                    'total_documents': len(clean_texts),
                    'embedding_dimension': embeddings.shape[1],
                    'inertia': kmeans.inertia_
                }
            }
            
        except Exception as e:
            return {'error': f'Embedding clustering hatasÄ±: {e}'}
    
    def _generate_topic_name(self, top_words: List[str]) -> str:
        """Konu iÃ§in otomatik isim oluÅŸtur"""
        if not top_words:
            return "Belirsiz Konu"
        
        # TÃ¼rkÃ§e konu isimleri iÃ§in Ã¶zel kural seti
        topic_patterns = {
            ('kargo', 'teslimat', 'paket', 'gÃ¶nderi'): 'Kargo & Teslimat',
            ('kalite', 'saÄŸlam', 'dayanÄ±klÄ±', 'bozuk'): 'ÃœrÃ¼n Kalitesi',
            ('fiyat', 'ucuz', 'pahalÄ±', 'uygun'): 'Fiyat & DeÄŸer',
            ('beden', 'bÃ¼yÃ¼k', 'kÃ¼Ã§Ã¼k', 'uyum'): 'Beden & Uyum',
            ('renk', 'gÃ¶rsel', 'resim', 'fotoÄŸraf'): 'GÃ¶rsel & Renk',
            ('hizmet', 'mÃ¼ÅŸteri', 'destek', 'yardÄ±m'): 'MÃ¼ÅŸteri Hizmeti',
            ('hÄ±zlÄ±', 'yavaÅŸ', 'zaman', 'geÃ§'): 'HÄ±z & Zaman',
            ('gÃ¼zel', 'beÄŸen', 'memnun', 'tavsiye'): 'Genel Memnuniyet',
            ('problem', 'sorun', 'ÅŸikayet', 'kÃ¶tÃ¼'): 'Sorunlar & Åikayetler'
        }
        
        # Pattern matching
        top_words_set = set(top_words[:5])
        for pattern, name in topic_patterns.items():
            if any(word in top_words_set for word in pattern):
                return name
        
        # Pattern bulunamazsa ilk 3 kelimeyi kullan
        return ' & '.join(top_words[:3]).title()
    
    def analyze_topics(self, comments: List[Dict], lda_topics: int = 6, cluster_topics: int = 6) -> Dict:
        """KapsamlÄ± konu analizi"""
        texts = [comment.get('comment', '') for comment in comments if comment.get('comment', '').strip()]
        
        if len(texts) < 10:
            return {'error': 'En az 10 yorum gerekli'}
        
        print(f"ğŸ” {len(texts)} yorum Ã¼zerinde konu analizi baÅŸlÄ±yor...")
        
        results = {
            'total_comments': len(texts),
            'lda_analysis': {},
            'clustering_analysis': {},
            'comparison': {},
            'recommendations': []
        }
        
        # LDA analizi
        lda_result = self.lda_topic_modeling(texts, n_topics=lda_topics)
        results['lda_analysis'] = lda_result
        
        # Embedding clustering
        clustering_result = self.embedding_clustering(texts, n_clusters=cluster_topics)
        results['clustering_analysis'] = clustering_result
        
        # KarÅŸÄ±laÅŸtÄ±rma ve Ã¶neriler
        results['comparison'] = self._compare_methods(lda_result, clustering_result)
        results['recommendations'] = self._generate_recommendations(lda_result, clustering_result)
        
        return results
    
    def _compare_methods(self, lda_result: Dict, clustering_result: Dict) -> Dict:
        """Ä°ki metodu karÅŸÄ±laÅŸtÄ±r"""
        comparison = {
            'method_comparison': {
                'LDA': {
                    'strength': 'Ä°statistiksel konu Ã§Ä±karÄ±mÄ±, kelime aÄŸÄ±rlÄ±klarÄ±',
                    'weakness': 'KÄ±sa metinlerde daha az etkili'
                },
                'Clustering': {
                    'strength': 'Semantik benzerlik, kÄ±sa metinlerde etkili',
                    'weakness': 'Daha fazla hesaplama gÃ¼cÃ¼ gerektirir'
                }
            }
        }
        
        # Her iki metodda da ortak konularÄ± bul
        if 'topics' in lda_result and 'clusters' in clustering_result:
            lda_topics = [topic['topic_name'] for topic in lda_result['topics']]
            cluster_topics = [cluster['topic_name'] for cluster in clustering_result['clusters']]
            
            comparison['common_themes'] = list(set(lda_topics) & set(cluster_topics))
            comparison['lda_unique'] = list(set(lda_topics) - set(cluster_topics))
            comparison['clustering_unique'] = list(set(cluster_topics) - set(lda_topics))
        
        return comparison
    
    def _generate_recommendations(self, lda_result: Dict, clustering_result: Dict) -> List[str]:
        """Analiz sonuÃ§larÄ±na gÃ¶re Ã¶neriler oluÅŸtur"""
        recommendations = []
        
        # LDA sonuÃ§larÄ±na gÃ¶re Ã¶neriler
        if 'topics' in lda_result:
            for topic in lda_result['topics']:
                top_word = topic['words'][0] if topic['words'] else ''
                if 'kargo' in top_word or 'teslimat' in top_word:
                    recommendations.append("ğŸšš Kargo/teslimat konusu Ã¶nemli - lojistik sÃ¼reÃ§leri gÃ¶zden geÃ§irin")
                elif 'kalite' in top_word or 'bozuk' in top_word:
                    recommendations.append("ğŸ”§ ÃœrÃ¼n kalitesi konusu Ã¶nemli - QC sÃ¼reÃ§lerini gÃ¼Ã§lendirin")
                elif 'fiyat' in top_word or 'pahalÄ±' in top_word:
                    recommendations.append("ğŸ’° Fiyat konusu Ã¶nemli - fiyatlandÄ±rma stratejinizi gÃ¶zden geÃ§irin")
        
        # Clustering sonuÃ§larÄ±na gÃ¶re Ã¶neriler
        if 'clusters' in clustering_result:
            large_clusters = [c for c in clustering_result['clusters'] if c['size'] > len(clustering_result['clusters']) * 0.2]
            for cluster in large_clusters:
                recommendations.append(f"ğŸ“Š '{cluster['topic_name']}' konusu yorumlarÄ±n %{round(cluster['size']/clustering_result['model_info']['total_documents']*100)}ini oluÅŸturuyor")
        
        if not recommendations:
            recommendations.append("ğŸ“ˆ Yorum sayÄ±sÄ± az, daha fazla veri toplandÄ±ktan sonra detaylÄ± analiz yapÄ±labilir")
        
        return recommendations
    
    def generate_topic_report(self, analysis_results: Dict) -> str:
        """DetaylÄ± konu analizi raporu oluÅŸtur"""
        report = []
        report.append("ğŸ” OTOMATIK KONU Ã‡IKARIMI RAPORU")
        report.append("=" * 60)
        
        total_comments = analysis_results.get('total_comments', 0)
        report.append(f"\nğŸ“Š Analiz Edilen Yorum SayÄ±sÄ±: {total_comments}")
        
        # LDA SonuÃ§larÄ±
        lda_result = analysis_results.get('lda_analysis', {})
        if 'topics' in lda_result:
            report.append(f"\nğŸ¤– LDA KONU ANALÄ°ZÄ°")
            report.append("-" * 40)
            
            for topic in lda_result['topics']:
                report.append(f"\nğŸ“‹ Konu {topic['topic_id'] + 1}: {topic['topic_name']}")
                report.append(f"   ğŸ”‘ Anahtar Kelimeler: {', '.join(topic['words'][:5])}")
        
        # Clustering SonuÃ§larÄ±
        clustering_result = analysis_results.get('clustering_analysis', {})
        if 'clusters' in clustering_result:
            report.append(f"\nğŸ§  EMBEDDING CLUSTERING ANALÄ°ZÄ°")
            report.append("-" * 40)
            
            for cluster in clustering_result['clusters']:
                percentage = round(cluster['size'] / clustering_result['model_info']['total_documents'] * 100, 1)
                report.append(f"\nğŸ“Š KÃ¼me {cluster['cluster_id'] + 1}: {cluster['topic_name']}")
                report.append(f"   ğŸ“ˆ Yorum SayÄ±sÄ±: {cluster['size']} (%{percentage})")
                report.append(f"   ğŸ”‘ Anahtar Kelimeler: {', '.join(cluster['top_words'][:5])}")
                report.append(f"   ğŸ’¬ Ã–rnek: {cluster['representative_text']}")
        
        # KarÅŸÄ±laÅŸtÄ±rma
        comparison = analysis_results.get('comparison', {})
        if 'common_themes' in comparison and comparison['common_themes']:
            report.append(f"\nğŸ”„ ORTAK KONULAR")
            report.append("-" * 40)
            for theme in comparison['common_themes']:
                report.append(f"   âœ… {theme}")
        
        # Ã–neriler
        recommendations = analysis_results.get('recommendations', [])
        if recommendations:
            report.append(f"\nğŸ’¡ Ã–NERÄ°LER")
            report.append("-" * 40)
            for rec in recommendations:
                report.append(f"   {rec}")
        
        return '\n'.join(report)
    
    def save_topic_analysis(self, analysis_results: Dict, filename: str = 'topic_analysis.json'):
        """Konu analizi sonuÃ§larÄ±nÄ± kaydet"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
            print(f"ğŸ“ Konu analizi {filename} dosyasÄ±na kaydedildi")
        except Exception as e:
            print(f"âŒ Kaydetme hatasÄ±: {e}")
    
    def save_topic_report(self, analysis_results: Dict, filename: str = 'topic_report.txt'):
        """Konu analizi raporunu metin olarak kaydet"""
        try:
            report = self.generate_topic_report(analysis_results)
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“„ Konu raporu {filename} dosyasÄ±na kaydedildi")
        except Exception as e:
            print(f"âŒ Rapor kaydetme hatasÄ±: {e}")


def main():
    """Topic modeling Ã¶rnek kullanÄ±m"""
    analyzer = TopicModelingAnalyzer()
    
    # YorumlarÄ± yÃ¼kle
    comments = analyzer.load_comments_from_csv("trendyol_comments.csv")
    
    if not comments:
        print("âŒ Yorum yÃ¼klenemedi!")
        return
    
    if len(comments) < 10:
        print("âš ï¸ En az 10 yorum gerekli!")
        return
    
    print("\nğŸš€ OTOMATIK KONU Ã‡IKARIMÄ° SÄ°STEMÄ°")
    print("=" * 50)
    
    # Konu analizini yap
    analysis_results = analyzer.analyze_topics(comments, lda_topics=6, cluster_topics=6)
    
    if 'error' in analysis_results:
        print(f"âŒ Analiz hatasÄ±: {analysis_results['error']}")
        return
    
    # Raporu gÃ¶ster
    report = analyzer.generate_topic_report(analysis_results)
    print("\n" + report)
    
    # SonuÃ§larÄ± kaydet
    analyzer.save_topic_analysis(analysis_results, 'topic_analysis.json')
    analyzer.save_topic_report(analysis_results, 'topic_report.txt')
    
    print(f"\nâœ… Analiz tamamlandÄ±!")
    print(f"ğŸ“ DetaylÄ± sonuÃ§lar: topic_analysis.json")
    print(f"ğŸ“„ Rapor: topic_report.txt")


if __name__ == "__main__":
    main() 