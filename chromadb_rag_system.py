#!/usr/bin/env python3
"""
üöÄ FAISS + Sentence Transformers RAG Sistemi
Geli≈ümi≈ü vector search ile semantic similarity
"""

import os
import json
import pickle
import hashlib
import numpy as np
import pandas as pd
from datetime import datetime
from typing import List, Dict, Any, Optional
import sqlite3

# FAISS ve embedding imports
import faiss
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Mevcut analiz mod√ºlleri
try:
    from advanced_comment_analyzer import AdvancedCommentAnalyzer
    from priority_analyzer import PriorityAnalyzer
    ANALYZERS_AVAILABLE = True
except ImportError:
    ANALYZERS_AVAILABLE = False

class FaissRAGSystem:
    def __init__(
        self,
        model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        db_path: str = "faiss_rag.db",
        vector_path: str = "./faiss_vectors"
    ):
        """
        FAISS tabanlƒ± RAG sistemi
        
        Args:
            model_name: Sentence transformer model
            db_path: SQLite veritabanƒ± yolu
            vector_path: FAISS indeks dosyalarƒ±nƒ±n yolu
        """
        self.model_name = model_name
        self.db_path = db_path
        self.vector_path = vector_path
        
        # Embedding modeli
        print("üì• Embedding modeli y√ºkleniyor...")
        self.embedding_model = SentenceTransformer(model_name)
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        print(f"‚úÖ Model y√ºklendi: {model_name} (dim: {self.embedding_dim})")
        
        # FAISS indeks
        self.comment_index = None
        self.knowledge_index = None
        self.comment_metadata = []
        self.knowledge_metadata = []
        
        # Vector path olu≈ütur
        os.makedirs(vector_path, exist_ok=True)
        
        # SQLite database
        self.init_database()
        
        # Analiz mod√ºlleri
        if ANALYZERS_AVAILABLE:
            self.comment_analyzer = AdvancedCommentAnalyzer()
            self.priority_analyzer = PriorityAnalyzer()
            print("‚úÖ Geli≈ümi≈ü analiz mod√ºlleri aktif")
        else:
            print("‚ö†Ô∏è Temel mod - geli≈ümi≈ü analiz mod√ºlleri yok")
        
        # Mevcut indeksleri y√ºkle
        self.load_indexes()
        
        print("üöÄ FAISS RAG sistemi hazƒ±r!")
    
    def init_database(self):
        """SQLite veritabanƒ±nƒ± ba≈ülat"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Yorumlar tablosu
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS comments (
                id INTEGER PRIMARY KEY,
                user TEXT,
                date TEXT,
                comment TEXT,
                comment_hash TEXT UNIQUE,
                category TEXT DEFAULT 'unknown',
                priority_score REAL DEFAULT 0,
                embedding_id INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Bilgi tabanƒ±
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS knowledge_base (
                id INTEGER PRIMARY KEY,
                category TEXT,
                problem TEXT,
                solution TEXT,
                keywords TEXT,
                embedding_id INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Vector indeks metadatasƒ±
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS vector_metadata (
                id INTEGER PRIMARY KEY,
                vector_type TEXT,  -- 'comment' or 'knowledge'
                record_id INTEGER,
                embedding_vector BLOB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        print("üìÅ SQLite veritabanƒ± hazƒ±r")
    
    def get_comment_hash(self, comment: str, user: str = "", date: str = "") -> str:
        """Yorum i√ßin unique hash"""
        text = f"{comment}{user}{date}"
        return hashlib.md5(text.encode()).hexdigest()
    
    def create_embeddings(self, texts: List[str]) -> np.ndarray:
        """Metinler i√ßin embedding olu≈ütur"""
        print(f"üîÑ {len(texts)} metin i√ßin embedding olu≈üturuluyor...")
        embeddings = self.embedding_model.encode(
            texts,
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        return embeddings.astype('float32')
    
    def load_comments_from_csv(self, csv_file: str = "trendyol_comments.csv") -> int:
        """CSV'den yorumlarƒ± y√ºkle ve vector store'a ekle"""
        if not os.path.exists(csv_file):
            print(f"‚ùå {csv_file} bulunamadƒ±!")
            return 0
        
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
            print(f"üìä {len(df)} yorum bulundu")
        except Exception as e:
            print(f"‚ùå CSV okuma hatasƒ±: {e}")
            return 0
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Yeni yorumlarƒ± hazƒ±rla
        new_comments = []
        comment_texts = []
        
        for _, row in df.iterrows():
            comment = str(row.get('comment', ''))
            user = str(row.get('user', ''))
            date = str(row.get('date', ''))
            
            if not comment.strip():
                continue
            
            comment_hash = self.get_comment_hash(comment, user, date)
            
            # Zaten var mƒ± kontrol et
            cursor.execute('SELECT id FROM comments WHERE comment_hash = ?', (comment_hash,))
            if cursor.fetchone():
                continue
            
            # Analiz yap
            category = 'unknown'
            priority_score = 0
            
            if ANALYZERS_AVAILABLE:
                try:
                    analysis = self.comment_analyzer.analyze_comment_categories(comment)
                    # En y√ºksek confidence'lƒ± kategoriyi bul
                    highest_category = 'unknown'
                    highest_confidence = 0
                    for cat, result in analysis.items():
                        if result.get('relevant') and result.get('confidence', 0) > highest_confidence:
                            highest_category = cat
                            highest_confidence = result['confidence']
                    
                    category = highest_category
                    priority_score = self.priority_analyzer.calculate_negativity_score(comment).get('negativity_score', 0)
                except Exception as e:
                    print(f"‚ö†Ô∏è Analiz hatasƒ±: {e}")
            
            new_comments.append((user, date, comment, comment_hash, category, priority_score))
            comment_texts.append(comment)
        
        if not new_comments:
            print("üìù T√ºm yorumlar zaten mevcut")
            conn.close()
            return 0
        
        # Embeddings olu≈ütur
        embeddings = self.create_embeddings(comment_texts)
        
        # FAISS indekse ekle
        if self.comment_index is None:
            self.comment_index = faiss.IndexFlatIP(self.embedding_dim)  # Inner Product (cosine similarity)
        
        start_id = len(self.comment_metadata)
        self.comment_index.add(embeddings)
        
        # Veritabanƒ±na kaydet
        for i, (user, date, comment, comment_hash, category, priority_score) in enumerate(new_comments):
            embedding_id = start_id + i
            
            # Comment tablosuna ekle
            cursor.execute('''
                INSERT INTO comments 
                (user, date, comment, comment_hash, category, priority_score, embedding_id)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (user, date, comment, comment_hash, category, priority_score, embedding_id))
            
            comment_id = cursor.lastrowid
            
            # Metadata ekle
            self.comment_metadata.append({
                'id': comment_id,
                'user': user,
                'date': date,
                'comment': comment,
                'category': category,
                'priority_score': priority_score,
                'embedding_id': embedding_id
            })
            
            # Vector metadata kaydet
            embedding_blob = embeddings[i].tobytes()
            cursor.execute('''
                INSERT INTO vector_metadata (vector_type, record_id, embedding_vector)
                VALUES (?, ?, ?)
            ''', ('comment', comment_id, embedding_blob))
        
        conn.commit()
        conn.close()
        
        # ƒ∞ndeksi kaydet
        self.save_indexes()
        
        print(f"‚úÖ {len(new_comments)} yeni yorum eklendi")
        return len(new_comments)
    
    def add_knowledge(self, category: str, problem: str, solution: str, keywords: List[str] = None):
        """Bilgi tabanƒ±na entry ekle"""
        if keywords is None:
            keywords = []
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Bilgi kaydƒ±
        cursor.execute('''
            INSERT INTO knowledge_base (category, problem, solution, keywords)
            VALUES (?, ?, ?, ?)
        ''', (category, problem, solution, ','.join(keywords)))
        
        knowledge_id = cursor.lastrowid
        
        # Embedding olu≈ütur (problem + solution birle≈üimi)
        text = f"{problem} {solution}"
        embedding = self.create_embeddings([text])[0]
        
        # FAISS indekse ekle
        if self.knowledge_index is None:
            self.knowledge_index = faiss.IndexFlatIP(self.embedding_dim)
        
        embedding_id = len(self.knowledge_metadata)
        self.knowledge_index.add(embedding.reshape(1, -1))
        
        # Metadata g√ºncelle
        cursor.execute('UPDATE knowledge_base SET embedding_id = ? WHERE id = ?', 
                      (embedding_id, knowledge_id))
        
        self.knowledge_metadata.append({
            'id': knowledge_id,
            'category': category,
            'problem': problem,
            'solution': solution,
            'keywords': keywords,
            'embedding_id': embedding_id
        })
        
        # Vector metadata kaydet
        embedding_blob = embedding.tobytes()
        cursor.execute('''
            INSERT INTO vector_metadata (vector_type, record_id, embedding_vector)
            VALUES (?, ?, ?)
        ''', ('knowledge', knowledge_id, embedding_blob))
        
        conn.commit()
        conn.close()
        
        # ƒ∞ndeksi kaydet
        self.save_indexes()
        
        print(f"üìö Bilgi eklendi: {category} - {problem[:50]}...")
    
    def search_similar_comments(
        self,
        query: str,
        limit: int = 5,
        similarity_threshold: float = 0.3,
        sentiment_filter: str = None  # 'positive', 'negative', 'neutral' veya None
    ) -> List[Dict]:
        """FAISS ile benzer yorumlarƒ± ara"""
        
        if self.comment_index is None or len(self.comment_metadata) == 0:
            return []
        
        # Query embedding
        query_embedding = self.create_embeddings([query])[0]
        
        # Embedding boyutunu kontrol et
        if query_embedding.shape[0] != self.embedding_dim:
            print(f"‚ö†Ô∏è Embedding boyut uyumsuzluƒüu: {query_embedding.shape[0]} != {self.embedding_dim}")
            return []
        
        # FAISS arama (daha fazla sonu√ß al filtering i√ßin)
        search_limit = min(limit * 4, len(self.comment_metadata))
        try:
            scores, indices = self.comment_index.search(
                query_embedding.reshape(1, -1), 
                search_limit
            )
        except Exception as e:
            print(f"‚ö†Ô∏è FAISS arama hatasƒ±: {e}")
            print("üîÑ ƒ∞ndeksleri yeniden olu≈üturuyor...")
            self.reset_vectors()
            return []
        
        results = []
        negative_keywords = ['sorun', 'problem', 'k√∂t√º', 'berbat', 'bozuk', 'defolu', '≈üikayet', 'memnun deƒüil', 
                           'kalitesiz', 'ge√ß', 'yava≈ü', 'hasarlƒ±', 'kƒ±rƒ±k', 'iade', 'beƒüenmedim', 'beƒüenmedik',
                           'tavsiye etmem', 'pi≈üman', 'hayal kƒ±rƒ±klƒ±ƒüƒ±', 'rezalet', '√ß√∂p', 'para israfƒ±', 
                           'aldatmaca', 'sahte', 'taklit']
        
        positive_keywords = ['beƒüendik', 'beƒüendim', 'm√ºkemmel', 'harika', 's√ºper', 'tavsiye ederim', 
                           'memnun', 'kaliteli', 'g√ºzel', 'ba≈üarƒ±lƒ±', 'te≈üekk√ºr', 'stok', 'vazge√ßilmez',
                           'favorim', 'severek', 'mutlu', '√ßok iyi']
        
        for score, idx in zip(scores[0], indices[0]):
            if idx == -1 or score < similarity_threshold:
                continue
                
            if idx < len(self.comment_metadata):
                metadata = self.comment_metadata[idx]
                comment_text = metadata['comment'].lower()
                
                # Sentiment filtering - hem pozitif hem negatif kelimeleri kontrol et
                has_negative = any(keyword in comment_text for keyword in negative_keywords)
                has_positive = any(keyword in comment_text for keyword in positive_keywords)
                
                if sentiment_filter == 'negative':
                    # Sadece negatif kelimeler varsa VE pozitif kelimeler yoksa
                    if not has_negative or has_positive:
                        continue
                elif sentiment_filter == 'positive':
                    # Sadece pozitif kelimeler varsa VE negatif kelimeler yoksa
                    if not has_positive or has_negative:
                        continue
                
                results.append({
                    'id': metadata['id'],
                    'user': metadata['user'],
                    'date': metadata['date'],
                    'comment': metadata['comment'],
                    'category': metadata['category'],
                    'priority_score': metadata['priority_score'],
                    'similarity': float(score)
                })
        
        # Similarity'ye g√∂re sƒ±rala ve limit uygula
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:limit]
    
    def search_knowledge_base(
        self,
        query: str,
        limit: int = 3,
        similarity_threshold: float = 0.3
    ) -> List[Dict]:
        """FAISS ile bilgi tabanƒ±nda ara"""
        
        if self.knowledge_index is None or len(self.knowledge_metadata) == 0:
            return []
        
        # Query embedding
        query_embedding = self.create_embeddings([query])[0]
        
        # FAISS arama
        scores, indices = self.knowledge_index.search(
            query_embedding.reshape(1, -1),
            min(limit * 2, len(self.knowledge_metadata))
        )
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx == -1 or score < similarity_threshold:
                continue
                
            if idx < len(self.knowledge_metadata):
                metadata = self.knowledge_metadata[idx]
                results.append({
                    'id': metadata['id'],
                    'category': metadata['category'],
                    'problem': metadata['problem'],
                    'solution': metadata['solution'],
                    'keywords': metadata['keywords'],
                    'similarity': float(score)
                })
        
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:limit]
    
    def query(self, question: str) -> Dict[str, Any]:
        """Geli≈ümi≈ü RAG sorgusu"""
        print(f"üîç Soru: {question}")
        
        # Sorunun negatif/pozitif eƒüilimini tespit et
        question_lower = question.lower()
        problem_keywords = ['sorun', 'problem', '≈üikayet', 'k√∂t√º', 'berbat', 'nasƒ±l √ß√∂z√ºlebilir', 'iyile≈ütir']
        is_problem_query = any(keyword in question_lower for keyword in problem_keywords)
        
        # 1. Benzer yorumlarƒ± bul (sorun arayƒ±≈üƒ±ndaysa negatif filtrele)
        if is_problem_query:
            similar_comments = self.search_similar_comments(question, limit=3, sentiment_filter='negative')
            if not similar_comments:
                # Negatif bulamazsa genel arama yap
                similar_comments = self.search_similar_comments(question, limit=3)
        else:
            similar_comments = self.search_similar_comments(question, limit=3)
        
        # 2. Bilgi tabanƒ±nƒ± ara
        knowledge_results = self.search_knowledge_base(question, limit=2)
        
        # 3. Geli≈ümi≈ü yanƒ±t olu≈ütur
        answer_parts = []
        
        if similar_comments:
            answer_parts.append("**üîç Benzer Yorumlardan Bulgular:**")
            for i, comment in enumerate(similar_comments, 1):
                priority = comment['priority_score']
                category = comment['category']
                similarity = comment['similarity']
                
                # Sentiment tespiti
                comment_text = comment['comment'].lower()
                negative_keywords = ['sorun', 'problem', 'k√∂t√º', 'berbat', 'bozuk', 'defolu', '≈üikayet', 'memnun deƒüil', 
                                   'kalitesiz', 'ge√ß', 'yava≈ü', 'hasarlƒ±', 'kƒ±rƒ±k', 'iade', 'beƒüenmedim', 'beƒüenmedik',
                                   'tavsiye etmem', 'pi≈üman', 'hayal kƒ±rƒ±klƒ±ƒüƒ±', 'rezalet', '√ß√∂p', 'para israfƒ±']
                positive_keywords = ['beƒüendik', 'beƒüendim', 'm√ºkemmel', 'harika', 's√ºper', 'tavsiye ederim', 
                                   'memnun', 'kaliteli', 'g√ºzel', 'ba≈üarƒ±lƒ±', 'te≈üekk√ºr', 'stok', 'vazge√ßilmez',
                                   'favorim', 'severek', 'mutlu', '√ßok iyi']
                
                has_negative = any(kw in comment_text for kw in negative_keywords)
                has_positive = any(kw in comment_text for kw in positive_keywords)
                
                if has_negative and not has_positive:
                    sentiment_emoji = "üî¥"
                elif has_positive and not has_negative:
                    sentiment_emoji = "üü¢"
                else:
                    sentiment_emoji = "‚ö™"
                
                answer_parts.append(
                    f"{i}. {sentiment_emoji} {comment['comment'][:120]}... "
                    f"(Benzerlik: {similarity:.2f}, √ñncelik: {priority:.0f}/100, "
                    f"Kategori: {category})"
                )
        else:
            if is_problem_query:
                answer_parts.append("**üîç Benzer Yorumlardan Bulgular:**")
                answer_parts.append("Bu konuda olumsuz bir yorum tespit edilmedi. Bu olumlu bir durumdur! ‚úÖ")
        
        if knowledge_results:
            answer_parts.append("\n**üí° Bilgi Tabanƒ±ndan √á√∂z√ºm √ñnerileri:**")
            for i, kb in enumerate(knowledge_results, 1):
                similarity = kb['similarity']
                answer_parts.append(
                    f"{i}. **{kb['category'].title()}** (Benzerlik: {similarity:.2f})\n"
                    f"   Problem: {kb['problem']}\n"
                    f"   √á√∂z√ºm: {kb['solution']}"
                )
        
        if not answer_parts:
            answer_parts.append("Bu soru i√ßin ilgili bilgi bulunamadƒ±. Daha spesifik terimler deneyebilirsiniz.")
        
        answer = "\n".join(answer_parts)
        
        return {
            "question": question,
            "answer": answer,
            "similar_comments": similar_comments,
            "knowledge_results": knowledge_results,
            "embedding_model": self.model_name,
            "timestamp": datetime.now().isoformat(),
            "query_type": "problem_focused" if is_problem_query else "general"
        }
    
    def save_indexes(self):
        """FAISS indekslerini kaydet"""
        try:
            if self.comment_index is not None:
                faiss.write_index(self.comment_index, f"{self.vector_path}/comment_index.faiss")
                
                with open(f"{self.vector_path}/comment_metadata.pkl", 'wb') as f:
                    pickle.dump(self.comment_metadata, f)
            
            if self.knowledge_index is not None:
                faiss.write_index(self.knowledge_index, f"{self.vector_path}/knowledge_index.faiss")
                
                with open(f"{self.vector_path}/knowledge_metadata.pkl", 'wb') as f:
                    pickle.dump(self.knowledge_metadata, f)
            
            print("üíæ FAISS indeksleri kaydedildi")
        except Exception as e:
            print(f"‚ö†Ô∏è ƒ∞ndeks kaydetme hatasƒ±: {e}")
    
    def load_indexes(self):
        """FAISS indekslerini y√ºkle"""
        try:
            # Comment indeks
            comment_index_path = f"{self.vector_path}/comment_index.faiss"
            comment_metadata_path = f"{self.vector_path}/comment_metadata.pkl"
            
            if os.path.exists(comment_index_path) and os.path.exists(comment_metadata_path):
                self.comment_index = faiss.read_index(comment_index_path)
                with open(comment_metadata_path, 'rb') as f:
                    self.comment_metadata = pickle.load(f)
                print(f"üì• Comment indeks y√ºklendi: {len(self.comment_metadata)} yorum")
            
            # Knowledge indeks
            knowledge_index_path = f"{self.vector_path}/knowledge_index.faiss"
            knowledge_metadata_path = f"{self.vector_path}/knowledge_metadata.pkl"
            
            if os.path.exists(knowledge_index_path) and os.path.exists(knowledge_metadata_path):
                self.knowledge_index = faiss.read_index(knowledge_index_path)
                with open(knowledge_metadata_path, 'rb') as f:
                    self.knowledge_metadata = pickle.load(f)
                print(f"üì• Knowledge indeks y√ºklendi: {len(self.knowledge_metadata)} bilgi")
        
        except Exception as e:
            print(f"‚ö†Ô∏è ƒ∞ndeks y√ºkleme hatasƒ±: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """Sistem istatistikleri"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM comments')
        total_comments = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM knowledge_base')
        total_knowledge = cursor.fetchone()[0]
        
        cursor.execute('SELECT category, COUNT(*) FROM comments GROUP BY category')
        category_dist = dict(cursor.fetchall())
        
        cursor.execute('SELECT AVG(priority_score) FROM comments WHERE priority_score > 0')
        avg_priority = cursor.fetchone()[0] or 0
        
        conn.close()
        
        return {
            "total_comments": total_comments,
            "total_knowledge_entries": total_knowledge,
            "category_distribution": category_dist,
            "average_priority": round(avg_priority, 2),
            "vector_comments": len(self.comment_metadata),
            "vector_knowledge": len(self.knowledge_metadata),
            "embedding_model": self.model_name,
            "embedding_dimension": self.embedding_dim,
            "has_advanced_analyzers": ANALYZERS_AVAILABLE
        }
    
    def reset_vectors(self):
        """Vector store'u sƒ±fƒ±rla"""
        try:
            # FAISS indekslerini sƒ±fƒ±rla
            self.comment_index = None
            self.knowledge_index = None
            self.comment_metadata = []
            self.knowledge_metadata = []
            
            # Dosyalarƒ± sil
            for filename in os.listdir(self.vector_path):
                if filename.endswith(('.faiss', '.pkl')):
                    os.remove(os.path.join(self.vector_path, filename))
            
            # Veritabanƒ±nƒ± sƒ±fƒ±rla
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('DELETE FROM comments')
            cursor.execute('DELETE FROM knowledge_base')
            cursor.execute('DELETE FROM vector_metadata')
            conn.commit()
            conn.close()
            
            print("üóëÔ∏è Vector store sƒ±fƒ±rlandƒ±")
        except Exception as e:
            print(f"‚ö†Ô∏è Sƒ±fƒ±rlama hatasƒ±: {e}")


def setup_demo_knowledge(rag):
    """Demo bilgi tabanƒ±nƒ± kur"""
    print("üåê Demo bilgi tabanƒ± kuruluyor...")
    
    knowledge_entries = [
        {
            "category": "kargo",
            "problem": "Kargo gecikmesi, teslimat sorunlarƒ± ve paket hasarƒ±",
            "solution": "1) Kargo firmasƒ± √ße≈üitliliƒüi artƒ±rƒ±n, 2) Express teslimat se√ßeneƒüi ekleyin, 3) Paket takip sistemi geli≈ütirin, 4) Hasar garantisi verin",
            "keywords": ["kargo", "teslimat", "gecikme", "ge√ß", "hasar", "kƒ±rƒ±k"]
        },
        {
            "category": "kalite",
            "problem": "√úr√ºn kalitesi, dayanƒ±klƒ±lƒ±k ve bozuk √ºr√ºn ≈üikayetleri",
            "solution": "1) Kalite kontrol s√ºre√ßlerini g√º√ßlendirin, 2) Tedarik√ßi denetimi yapƒ±n, 3) Malzeme standartlarƒ±nƒ± y√ºkseltin, 4) Test prosed√ºrleri uygulayƒ±n",
            "keywords": ["kalite", "bozuk", "defolu", "k√∂t√º", "dayanƒ±ksƒ±z", "sahte"]
        },
        {
            "category": "beden",
            "problem": "Beden uyumsuzluƒüu, kalƒ±p sorunlarƒ± ve √∂l√ß√º farklƒ±lƒ±klarƒ±",
            "solution": "1) Beden tablosunu standart hale getirin, 2) AR deneme √∂zelliƒüi ekleyin, 3) Kullanƒ±cƒ± yorumlarƒ±ndan beden √∂nerileri sunun, 4) Esnek iade politikasƒ±",
            "keywords": ["beden", "uyum", "kalƒ±p", "b√ºy√ºk", "k√º√ß√ºk", "dar", "bol"]
        },
        {
            "category": "musteri_hizmetleri",
            "problem": "M√º≈üteri hizmetleri yanƒ±t s√ºresi, √ß√∂z√ºm kalitesi ve ileti≈üim sorunlarƒ±",
            "solution": "1) 7/24 canlƒ± destek kurun, 2) Yanƒ±t s√ºresini 2 saate d√º≈ü√ºr√ºn, 3) √áok dilli destek ekleyin, 4) Self-servis √ß√∂z√ºm merkezi olu≈üturun",
            "keywords": ["m√º≈üteri", "hizmet", "destek", "yanƒ±t", "√ß√∂z√ºm", "ilgisiz"]
        },
        {
            "category": "fiyat",
            "problem": "Fiyat algƒ±sƒ±, deƒüer kar≈üƒ±lƒ±ƒüƒ± ve rekabet sorunlarƒ±",
            "solution": "1) Dinamik fiyatlandƒ±rma sistemi kurun, 2) Sadakat programlarƒ± ba≈ülatƒ±n, 3) Deƒüer paketi olu≈üturun, 4) Fiyat kar≈üƒ±la≈ütƒ±rma aracƒ± ekleyin",
            "keywords": ["fiyat", "pahalƒ±", "deƒüer", "ucuz", "indirim", "kampanya"]
        },
        {
            "category": "website",
            "problem": "Website performansƒ±, kullanƒ±cƒ± deneyimi ve teknik sorunlar",
            "solution": "1) Sayfa y√ºkleme hƒ±zƒ±nƒ± artƒ±rƒ±n, 2) Mobil optimizasyonu geli≈ütirin, 3) Arama fonksiyonunu iyile≈ütirin, 4) Checkout s√ºrecini basitle≈ütirin",
            "keywords": ["site", "yava≈ü", "hata", "bulunamadƒ±", "mobil", "arama"]
        }
    ]
    
    for entry in knowledge_entries:
        rag.add_knowledge(
            entry["category"],
            entry["problem"],
            entry["solution"],
            entry["keywords"]
        )
    
    print("‚úÖ Demo bilgi tabanƒ± kuruldu")


def main():
    """Demo kullanƒ±mƒ±"""
    print("üöÄ FAISS RAG Sƒ∞STEMƒ∞ DEMO")
    print("=" * 60)
    
    # RAG sistemi olu≈ütur
    rag = FaissRAGSystem()
    
    # CSV'den yorumlarƒ± y√ºkle
    doc_count = rag.load_comments_from_csv("trendyol_comments.csv")
    if doc_count > 0:
        print(f"‚úÖ {doc_count} yeni yorum y√ºklendi")
    
    # Demo bilgi tabanƒ±nƒ± kur
    setup_demo_knowledge(rag)
    
    # Sistem istatistikleri
    stats = rag.get_stats()
    print(f"\nüìä Sƒ∞STEM ƒ∞STATƒ∞STƒ∞KLERƒ∞:")
    print(f"   üìù Toplam Yorum: {stats['total_comments']}")
    print(f"   üìö Bilgi Tabanƒ±: {stats['total_knowledge_entries']}")
    print(f"   üî¢ Vector Yorum: {stats['vector_comments']}")
    print(f"   üß† Embedding Model: {stats['embedding_model']}")
    print(f"   üìè Embedding Boyut: {stats['embedding_dimension']}")
    
    # Demo sorgularƒ±
    demo_questions = [
        "Kargo sorunlarƒ± hakkƒ±nda ≈üikayetler var mƒ±?",
        "√úr√ºn kalitesi ile ilgili en b√ºy√ºk problem nedir?",
        "Beden uyumsuzluƒüu nasƒ±l √ß√∂z√ºlebilir?",
        "M√º≈üteri hizmetleri nasƒ±l geli≈ütirilebilir?",
        "Website performansƒ± hakkƒ±nda ne d√º≈ü√ºn√ºl√ºyor?"
    ]
    
    print(f"\nüí¨ DEMO SORGULARI:")
    print("=" * 60)
    
    for question in demo_questions:
        print(f"\n‚ùì SORU: {question}")
        print("-" * 50)
        
        result = rag.query(question)
        print(f"ü§ñ YANIT:\n{result['answer']}")
        print(f"üìä Benzer yorum: {len(result['similar_comments'])}")
        print(f"üìö Bilgi sonucu: {len(result['knowledge_results'])}")
    
    print(f"\n‚úÖ DEMO TAMAMLANDI!")
    print("üí° Web aray√ºz√º i√ßin: streamlit run faiss_rag_streamlit.py")


if __name__ == "__main__":
    main() 