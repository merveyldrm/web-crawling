#!/usr/bin/env python3
"""
ğŸš€ FAISS + Sentence Transformers RAG Sistemi
GeliÅŸmiÅŸ vector search ile semantic similarity
"""

import os
import json
import pickle
import hashlib
import numpy as np
import pandas as pd
from datetime import datetime
from typing import List, Dict, Any, Optional
import sqlite3

# FAISS ve embedding imports
import faiss
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Mevcut analiz modÃ¼lleri
try:
    from advanced_comment_analyzer import AdvancedCommentAnalyzer
    from priority_analyzer import PriorityAnalyzer
    ANALYZERS_AVAILABLE = True
except ImportError:
    ANALYZERS_AVAILABLE = False

class FaissRAGSystem:
    def __init__(
        self,
        model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        db_path: str = "faiss_rag.db",
        vector_path: str = "./faiss_vectors"
    ):
        """
        FAISS tabanlÄ± RAG sistemi
        
        Args:
            model_name: Sentence transformer model
            db_path: SQLite veritabanÄ± yolu
            vector_path: FAISS indeks dosyalarÄ±nÄ±n yolu
        """
        self.model_name = model_name
        self.db_path = db_path
        self.vector_path = vector_path
        
        # Embedding modeli
        print("ğŸ“¥ Embedding modeli yÃ¼kleniyor...")
        self.embedding_model = SentenceTransformer(model_name)
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        print(f"âœ… Model yÃ¼klendi: {model_name} (dim: {self.embedding_dim})")
        
        # FAISS indeks
        self.comment_index = None
        self.knowledge_index = None
        self.comment_metadata = []
        self.knowledge_metadata = []
        
        # Vector path oluÅŸtur
        os.makedirs(vector_path, exist_ok=True)
        
        # SQLite database
        self.init_database()
        
        # Analiz modÃ¼lleri
        if ANALYZERS_AVAILABLE:
            self.comment_analyzer = AdvancedCommentAnalyzer()
            self.priority_analyzer = PriorityAnalyzer()
            print("âœ… GeliÅŸmiÅŸ analiz modÃ¼lleri aktif")
        else:
            print("âš ï¸ Temel mod - geliÅŸmiÅŸ analiz modÃ¼lleri yok")
        
        # Mevcut indeksleri yÃ¼kle
        self.load_indexes()
        
        print("ğŸš€ FAISS RAG sistemi hazÄ±r!")
    
    def init_database(self):
        """SQLite veritabanÄ±nÄ± baÅŸlat"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Yorumlar tablosu
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS comments (
                id INTEGER PRIMARY KEY,
                user TEXT,
                date TEXT,
                comment TEXT,
                comment_hash TEXT UNIQUE,
                category TEXT DEFAULT 'unknown',
                priority_score REAL DEFAULT 0,
                embedding_id INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Bilgi tabanÄ±
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS knowledge_base (
                id INTEGER PRIMARY KEY,
                category TEXT,
                problem TEXT,
                solution TEXT,
                keywords TEXT,
                embedding_id INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Vector indeks metadatasÄ±
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS vector_metadata (
                id INTEGER PRIMARY KEY,
                vector_type TEXT,  -- 'comment' or 'knowledge'
                record_id INTEGER,
                embedding_vector BLOB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        print("ğŸ“ SQLite veritabanÄ± hazÄ±r")
    
    def get_comment_hash(self, comment: str, user: str = "", date: str = "") -> str:
        """Yorum iÃ§in unique hash"""
        text = f"{comment}{user}{date}"
        return hashlib.md5(text.encode()).hexdigest()
    
    def create_embeddings(self, texts: List[str]) -> np.ndarray:
        """Metinler iÃ§in embedding oluÅŸtur"""
        print(f"ğŸ”„ {len(texts)} metin iÃ§in embedding oluÅŸturuluyor...")
        embeddings = self.embedding_model.encode(
            texts,
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        return embeddings.astype('float32')
    
    def load_comments_from_csv(self, csv_file: str = "trendyol_comments.csv") -> int:
        """CSV'den yorumlarÄ± yÃ¼kle ve vector store'a ekle"""
        if not os.path.exists(csv_file):
            print(f"âŒ {csv_file} bulunamadÄ±!")
            return 0
        
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
            print(f"ğŸ“Š {len(df)} yorum bulundu")
        except Exception as e:
            print(f"âŒ CSV okuma hatasÄ±: {e}")
            return 0
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Yeni yorumlarÄ± hazÄ±rla
        new_comments = []
        comment_texts = []
        
        for _, row in df.iterrows():
            comment = str(row.get('comment', ''))
            user = str(row.get('user', ''))
            date = str(row.get('date', ''))
            
            if not comment.strip():
                continue
            
            comment_hash = self.get_comment_hash(comment, user, date)
            
            # Zaten var mÄ± kontrol et
            cursor.execute('SELECT id FROM comments WHERE comment_hash = ?', (comment_hash,))
            if cursor.fetchone():
                continue
            
            # Analiz yap
            category = 'unknown'
            priority_score = 0
            
            if ANALYZERS_AVAILABLE:
                try:
                    analysis = self.comment_analyzer.analyze_comment_categories(comment)
                    # En yÃ¼ksek confidence'lÄ± kategoriyi bul
                    highest_category = 'unknown'
                    highest_confidence = 0
                    for cat, result in analysis.items():
                        if result.get('relevant') and result.get('confidence', 0) > highest_confidence:
                            highest_category = cat
                            highest_confidence = result['confidence']
                    
                    category = highest_category
                    priority_score = self.priority_analyzer.calculate_negativity_score(comment).get('negativity_score', 0)
                except Exception as e:
                    print(f"âš ï¸ Analiz hatasÄ±: {e}")
            
            new_comments.append((user, date, comment, comment_hash, category, priority_score))
            comment_texts.append(comment)
        
        if not new_comments:
            print("ğŸ“ TÃ¼m yorumlar zaten mevcut")
            conn.close()
            return 0
        
        # Embeddings oluÅŸtur
        embeddings = self.create_embeddings(comment_texts)
        
        # FAISS indekse ekle
        if self.comment_index is None:
            self.comment_index = faiss.IndexFlatIP(self.embedding_dim)  # Inner Product (cosine similarity)
        
        start_id = len(self.comment_metadata)
        self.comment_index.add(embeddings)
        
        # VeritabanÄ±na kaydet
        for i, (user, date, comment, comment_hash, category, priority_score) in enumerate(new_comments):
            embedding_id = start_id + i
            
            # Comment tablosuna ekle
            cursor.execute('''
                INSERT INTO comments 
                (user, date, comment, comment_hash, category, priority_score, embedding_id)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (user, date, comment, comment_hash, category, priority_score, embedding_id))
            
            comment_id = cursor.lastrowid
            
            # Metadata ekle
            self.comment_metadata.append({
                'id': comment_id,
                'user': user,
                'date': date,
                'comment': comment,
                'category': category,
                'priority_score': priority_score,
                'embedding_id': embedding_id
            })
            
            # Vector metadata kaydet
            embedding_blob = embeddings[i].tobytes()
            cursor.execute('''
                INSERT INTO vector_metadata (vector_type, record_id, embedding_vector)
                VALUES (?, ?, ?)
            ''', ('comment', comment_id, embedding_blob))
        
        conn.commit()
        conn.close()
        
        # Ä°ndeksi kaydet
        self.save_indexes()
        
        print(f"âœ… {len(new_comments)} yeni yorum eklendi")
        return len(new_comments)
    
    def add_knowledge(self, category: str, problem: str, solution: str, keywords: List[str] = None):
        """Bilgi tabanÄ±na entry ekle"""
        if keywords is None:
            keywords = []
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Bilgi kaydÄ±
        cursor.execute('''
            INSERT INTO knowledge_base (category, problem, solution, keywords)
            VALUES (?, ?, ?, ?)
        ''', (category, problem, solution, ','.join(keywords)))
        
        knowledge_id = cursor.lastrowid
        
        # Embedding oluÅŸtur (problem + solution birleÅŸimi)
        text = f"{problem} {solution}"
        embedding = self.create_embeddings([text])[0]
        
        # FAISS indekse ekle
        if self.knowledge_index is None:
            self.knowledge_index = faiss.IndexFlatIP(self.embedding_dim)
        
        embedding_id = len(self.knowledge_metadata)
        self.knowledge_index.add(embedding.reshape(1, -1))
        
        # Metadata gÃ¼ncelle
        cursor.execute('UPDATE knowledge_base SET embedding_id = ? WHERE id = ?', 
                      (embedding_id, knowledge_id))
        
        self.knowledge_metadata.append({
            'id': knowledge_id,
            'category': category,
            'problem': problem,
            'solution': solution,
            'keywords': keywords,
            'embedding_id': embedding_id
        })
        
        # Vector metadata kaydet
        embedding_blob = embedding.tobytes()
        cursor.execute('''
            INSERT INTO vector_metadata (vector_type, record_id, embedding_vector)
            VALUES (?, ?, ?)
        ''', ('knowledge', knowledge_id, embedding_blob))
        
        conn.commit()
        conn.close()
        
        # Ä°ndeksi kaydet
        self.save_indexes()
        
        print(f"ğŸ“š Bilgi eklendi: {category} - {problem[:50]}...")
    
    def search_similar_comments(
        self,
        query: str,
        limit: int = 5,
        similarity_threshold: float = 0.3,
        sentiment_filter: str = None  # 'positive', 'negative', 'neutral' veya None
    ) -> List[Dict]:
        """FAISS ile benzer yorumlarÄ± ara"""
        
        if self.comment_index is None or len(self.comment_metadata) == 0:
            return []
        
        # Query embedding
        query_embedding = self.create_embeddings([query])[0]
        
        # Embedding boyutunu kontrol et
        if query_embedding.shape[0] != self.embedding_dim:
            print(f"âš ï¸ Embedding boyut uyumsuzluÄŸu: {query_embedding.shape[0]} != {self.embedding_dim}")
            return []
        
        # FAISS arama (daha fazla sonuÃ§ al filtering iÃ§in)
        search_limit = min(limit * 4, len(self.comment_metadata))
        try:
            scores, indices = self.comment_index.search(
                query_embedding.reshape(1, -1), 
                search_limit
            )
        except Exception as e:
            print(f"âš ï¸ FAISS arama hatasÄ±: {e}")
            print("ğŸ”„ Ä°ndeksleri yeniden oluÅŸturuyor...")
            self.reset_vectors()
            return []
        
        results = []
        negative_keywords = ['sorun', 'problem', 'kÃ¶tÃ¼', 'berbat', 'bozuk', 'defolu', 'ÅŸikayet', 'memnun deÄŸil', 
                           'kalitesiz', 'geÃ§', 'yavaÅŸ', 'hasarlÄ±', 'kÄ±rÄ±k', 'iade', 'beÄŸenmedim', 'beÄŸenmedik',
                           'tavsiye etmem', 'piÅŸman', 'hayal kÄ±rÄ±klÄ±ÄŸÄ±', 'rezalet', 'Ã§Ã¶p', 'para israfÄ±', 
                           'aldatmaca', 'sahte', 'taklit']
        
        positive_keywords = ['beÄŸendik', 'beÄŸendim', 'mÃ¼kemmel', 'harika', 'sÃ¼per', 'tavsiye ederim', 
                           'memnun', 'kaliteli', 'gÃ¼zel', 'baÅŸarÄ±lÄ±', 'teÅŸekkÃ¼r', 'stok', 'vazgeÃ§ilmez',
                           'favorim', 'severek', 'mutlu', 'Ã§ok iyi']
        
        for score, idx in zip(scores[0], indices[0]):
            if idx == -1 or score < similarity_threshold:
                continue
                
            if idx < len(self.comment_metadata):
                metadata = self.comment_metadata[idx]
                comment_text = metadata['comment'].lower()
                
                # Sentiment filtering - hem pozitif hem negatif kelimeleri kontrol et
                has_negative = any(keyword in comment_text for keyword in negative_keywords)
                has_positive = any(keyword in comment_text for keyword in positive_keywords)
                
                if sentiment_filter == 'negative':
                    # Sadece negatif kelimeler varsa VE pozitif kelimeler yoksa
                    if not has_negative or has_positive:
                        continue
                elif sentiment_filter == 'positive':
                    # Sadece pozitif kelimeler varsa VE negatif kelimeler yoksa
                    if not has_positive or has_negative:
                        continue
                
                results.append({
                    'id': metadata['id'],
                    'user': metadata['user'],
                    'date': metadata['date'],
                    'comment': metadata['comment'],
                    'category': metadata['category'],
                    'priority_score': metadata['priority_score'],
                    'similarity': float(score)
                })
        
        # Similarity'ye gÃ¶re sÄ±rala ve limit uygula
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:limit]
    
    def search_knowledge_base(
        self,
        query: str,
        limit: int = 3,
        similarity_threshold: float = 0.3
    ) -> List[Dict]:
        """FAISS ile bilgi tabanÄ±nda ara"""
        
        if self.knowledge_index is None or len(self.knowledge_metadata) == 0:
            return []
        
        # Query embedding
        query_embedding = self.create_embeddings([query])[0]
        
        # FAISS arama
        scores, indices = self.knowledge_index.search(
            query_embedding.reshape(1, -1),
            min(limit * 2, len(self.knowledge_metadata))
        )
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx == -1 or score < similarity_threshold:
                continue
                
            if idx < len(self.knowledge_metadata):
                metadata = self.knowledge_metadata[idx]
                results.append({
                    'id': metadata['id'],
                    'category': metadata['category'],
                    'problem': metadata['problem'],
                    'solution': metadata['solution'],
                    'keywords': metadata['keywords'],
                    'similarity': float(score)
                })
        
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:limit]
    
    def query(self, question: str) -> Dict[str, Any]:
        """GeliÅŸmiÅŸ RAG sorgusu"""
        print(f"ğŸ” Soru: {question}")
        
        # Sorunun negatif/pozitif eÄŸilimini tespit et
        question_lower = question.lower()
        problem_keywords = ['sorun', 'problem', 'ÅŸikayet', 'kÃ¶tÃ¼', 'berbat', 'nasÄ±l Ã§Ã¶zÃ¼lebilir', 'iyileÅŸtir']
        is_problem_query = any(keyword in question_lower for keyword in problem_keywords)
        
        # 1. Benzer yorumlarÄ± bul (sorun arayÄ±ÅŸÄ±ndaysa negatif filtrele)
        if is_problem_query:
            similar_comments = self.search_similar_comments(question, limit=3, sentiment_filter='negative')
            if not similar_comments:
                # Negatif bulamazsa genel arama yap
                similar_comments = self.search_similar_comments(question, limit=3)
        else:
            similar_comments = self.search_similar_comments(question, limit=3)
        
        # 2. Bilgi tabanÄ±nÄ± ara
        knowledge_results = self.search_knowledge_base(question, limit=2)
        
        # 3. GeliÅŸmiÅŸ yanÄ±t oluÅŸtur
        answer_parts = []
        
        if similar_comments:
            answer_parts.append("**ğŸ” Benzer Yorumlardan Bulgular:**")
            for i, comment in enumerate(similar_comments, 1):
                priority = comment['priority_score']
                category = comment['category']
                similarity = comment['similarity']
                
                # Sentiment tespiti
                comment_text = comment['comment'].lower()
                negative_keywords = ['sorun', 'problem', 'kÃ¶tÃ¼', 'berbat', 'bozuk', 'defolu', 'ÅŸikayet', 'memnun deÄŸil', 
                                   'kalitesiz', 'geÃ§', 'yavaÅŸ', 'hasarlÄ±', 'kÄ±rÄ±k', 'iade', 'beÄŸenmedim', 'beÄŸenmedik',
                                   'tavsiye etmem', 'piÅŸman', 'hayal kÄ±rÄ±klÄ±ÄŸÄ±', 'rezalet', 'Ã§Ã¶p', 'para israfÄ±']
                positive_keywords = ['beÄŸendik', 'beÄŸendim', 'mÃ¼kemmel', 'harika', 'sÃ¼per', 'tavsiye ederim', 
                                   'memnun', 'kaliteli', 'gÃ¼zel', 'baÅŸarÄ±lÄ±', 'teÅŸekkÃ¼r', 'stok', 'vazgeÃ§ilmez',
                                   'favorim', 'severek', 'mutlu', 'Ã§ok iyi']
                
                has_negative = any(kw in comment_text for kw in negative_keywords)
                has_positive = any(kw in comment_text for kw in positive_keywords)
                
                if has_negative and not has_positive:
                    sentiment_emoji = "ğŸ”´"
                elif has_positive and not has_negative:
                    sentiment_emoji = "ğŸŸ¢"
                else:
                    sentiment_emoji = "âšª"
                
                answer_parts.append(
                    f"{i}. {sentiment_emoji} {comment['comment'][:120]}... "
                    f"(Benzerlik: {similarity:.2f}, Ã–ncelik: {priority:.0f}/100, "
                    f"Kategori: {category})"
                )
        else:
            if is_problem_query:
                answer_parts.append("**ğŸ” Benzer Yorumlardan Bulgular:**")
                answer_parts.append("Bu konuda olumsuz bir yorum tespit edilmedi. Bu olumlu bir durumdur! âœ…")
        
        if knowledge_results:
            answer_parts.append("\n**ğŸ’¡ Bilgi TabanÄ±ndan Ã‡Ã¶zÃ¼m Ã–nerileri:**")
            for i, kb in enumerate(knowledge_results, 1):
                similarity = kb['similarity']
                answer_parts.append(
                    f"{i}. **{kb['category'].title()}** (Benzerlik: {similarity:.2f})\n"
                    f"   Problem: {kb['problem']}\n"
                    f"   Ã‡Ã¶zÃ¼m: {kb['solution']}"
                )
        
        if not answer_parts:
            answer_parts.append("Bu soru iÃ§in ilgili bilgi bulunamadÄ±. Daha spesifik terimler deneyebilirsiniz.")
        
        answer = "\n".join(answer_parts)
        
        return {
            "question": question,
            "answer": answer,
            "similar_comments": similar_comments,
            "knowledge_results": knowledge_results,
            "embedding_model": self.model_name,
            "timestamp": datetime.now().isoformat(),
            "query_type": "problem_focused" if is_problem_query else "general"
        }
    
    def save_indexes(self):
        """FAISS indekslerini kaydet"""
        try:
            if self.comment_index is not None:
                faiss.write_index(self.comment_index, f"{self.vector_path}/comment_index.faiss")
                
                with open(f"{self.vector_path}/comment_metadata.pkl", 'wb') as f:
                    pickle.dump(self.comment_metadata, f)
            
            if self.knowledge_index is not None:
                faiss.write_index(self.knowledge_index, f"{self.vector_path}/knowledge_index.faiss")
                
                with open(f"{self.vector_path}/knowledge_metadata.pkl", 'wb') as f:
                    pickle.dump(self.knowledge_metadata, f)
            
            print("ğŸ’¾ FAISS indeksleri kaydedildi")
        except Exception as e:
            print(f"âš ï¸ Ä°ndeks kaydetme hatasÄ±: {e}")
    
    def load_indexes(self):
        """FAISS indekslerini yÃ¼kle"""
        try:
            # Comment indeks
            comment_index_path = f"{self.vector_path}/comment_index.faiss"
            comment_metadata_path = f"{self.vector_path}/comment_metadata.pkl"
            
            if os.path.exists(comment_index_path) and os.path.exists(comment_metadata_path):
                self.comment_index = faiss.read_index(comment_index_path)
                with open(comment_metadata_path, 'rb') as f:
                    self.comment_metadata = pickle.load(f)
                print(f"ğŸ“¥ Comment indeks yÃ¼klendi: {len(self.comment_metadata)} yorum")
            
            # Knowledge indeks
            knowledge_index_path = f"{self.vector_path}/knowledge_index.faiss"
            knowledge_metadata_path = f"{self.vector_path}/knowledge_metadata.pkl"
            
            if os.path.exists(knowledge_index_path) and os.path.exists(knowledge_metadata_path):
                self.knowledge_index = faiss.read_index(knowledge_index_path)
                with open(knowledge_metadata_path, 'rb') as f:
                    self.knowledge_metadata = pickle.load(f)
                print(f"ğŸ“¥ Knowledge indeks yÃ¼klendi: {len(self.knowledge_metadata)} bilgi")
        
        except Exception as e:
            print(f"âš ï¸ Ä°ndeks yÃ¼kleme hatasÄ±: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """Sistem istatistikleri"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM comments')
        total_comments = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM knowledge_base')
        total_knowledge = cursor.fetchone()[0]
        
        cursor.execute('SELECT category, COUNT(*) FROM comments GROUP BY category')
        category_dist = dict(cursor.fetchall())
        
        cursor.execute('SELECT AVG(priority_score) FROM comments WHERE priority_score > 0')
        avg_priority = cursor.fetchone()[0] or 0
        
        conn.close()
        
        return {
            "total_comments": total_comments,
            "total_knowledge_entries": total_knowledge,
            "category_distribution": category_dist,
            "average_priority": round(avg_priority, 2),
            "vector_comments": len(self.comment_metadata),
            "vector_knowledge": len(self.knowledge_metadata),
            "embedding_model": self.model_name,
            "embedding_dimension": self.embedding_dim,
            "has_advanced_analyzers": ANALYZERS_AVAILABLE
        }
    
    def reset_vectors(self):
        """Vector store'u sÄ±fÄ±rla"""
        try:
            # FAISS indekslerini sÄ±fÄ±rla
            self.comment_index = None
            self.knowledge_index = None
            self.comment_metadata = []
            self.knowledge_metadata = []
            
            # DosyalarÄ± sil
            for filename in os.listdir(self.vector_path):
                if filename.endswith(('.faiss', '.pkl')):
                    os.remove(os.path.join(self.vector_path, filename))
            
            # VeritabanÄ±nÄ± sÄ±fÄ±rla
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('DELETE FROM comments')
            cursor.execute('DELETE FROM knowledge_base')
            cursor.execute('DELETE FROM vector_metadata')
            conn.commit()
            conn.close()
            
            print("ğŸ—‘ï¸ Vector store sÄ±fÄ±rlandÄ±")
        except Exception as e:
            print(f"âš ï¸ SÄ±fÄ±rlama hatasÄ±: {e}")


def setup_demo_knowledge(rag):
    """Demo bilgi tabanÄ±nÄ± kur"""
    print("ğŸŒ Demo bilgi tabanÄ± kuruluyor...")
    
    knowledge_entries = [
        {
            "category": "kargo",
            "problem": "Kargo gecikmesi, teslimat sorunlarÄ± ve paket hasarÄ±",
            "solution": "1) Kargo firmasÄ± Ã§eÅŸitliliÄŸi artÄ±rÄ±n, 2) Express teslimat seÃ§eneÄŸi ekleyin, 3) Paket takip sistemi geliÅŸtirin, 4) Hasar garantisi verin",
            "keywords": ["kargo", "teslimat", "gecikme", "geÃ§", "hasar", "kÄ±rÄ±k"]
        },
        {
            "category": "kalite",
            "problem": "ÃœrÃ¼n kalitesi, dayanÄ±klÄ±lÄ±k ve bozuk Ã¼rÃ¼n ÅŸikayetleri",
            "solution": "1) Kalite kontrol sÃ¼reÃ§lerini gÃ¼Ã§lendirin, 2) TedarikÃ§i denetimi yapÄ±n, 3) Malzeme standartlarÄ±nÄ± yÃ¼kseltin, 4) Test prosedÃ¼rleri uygulayÄ±n",
            "keywords": ["kalite", "bozuk", "defolu", "kÃ¶tÃ¼", "dayanÄ±ksÄ±z", "sahte"]
        },
        {
            "category": "beden",
            "problem": "Beden uyumsuzluÄŸu, kalÄ±p sorunlarÄ± ve Ã¶lÃ§Ã¼ farklÄ±lÄ±klarÄ±",
            "solution": "1) Beden tablosunu standart hale getirin, 2) AR deneme Ã¶zelliÄŸi ekleyin, 3) KullanÄ±cÄ± yorumlarÄ±ndan beden Ã¶nerileri sunun, 4) Esnek iade politikasÄ±",
            "keywords": ["beden", "uyum", "kalÄ±p", "bÃ¼yÃ¼k", "kÃ¼Ã§Ã¼k", "dar", "bol"]
        },
        {
            "category": "musteri_hizmetleri",
            "problem": "MÃ¼ÅŸteri hizmetleri yanÄ±t sÃ¼resi, Ã§Ã¶zÃ¼m kalitesi ve iletiÅŸim sorunlarÄ±",
            "solution": "1) 7/24 canlÄ± destek kurun, 2) YanÄ±t sÃ¼resini 2 saate dÃ¼ÅŸÃ¼rÃ¼n, 3) Ã‡ok dilli destek ekleyin, 4) Self-servis Ã§Ã¶zÃ¼m merkezi oluÅŸturun",
            "keywords": ["mÃ¼ÅŸteri", "hizmet", "destek", "yanÄ±t", "Ã§Ã¶zÃ¼m", "ilgisiz"]
        },
        {
            "category": "fiyat",
            "problem": "Fiyat algÄ±sÄ±, deÄŸer karÅŸÄ±lÄ±ÄŸÄ± ve rekabet sorunlarÄ±",
            "solution": "1) Dinamik fiyatlandÄ±rma sistemi kurun, 2) Sadakat programlarÄ± baÅŸlatÄ±n, 3) DeÄŸer paketi oluÅŸturun, 4) Fiyat karÅŸÄ±laÅŸtÄ±rma aracÄ± ekleyin",
            "keywords": ["fiyat", "pahalÄ±", "deÄŸer", "ucuz", "indirim", "kampanya"]
        },
        {
            "category": "website",
            "problem": "Website performansÄ±, kullanÄ±cÄ± deneyimi ve teknik sorunlar",
            "solution": "1) Sayfa yÃ¼kleme hÄ±zÄ±nÄ± artÄ±rÄ±n, 2) Mobil optimizasyonu geliÅŸtirin, 3) Arama fonksiyonunu iyileÅŸtirin, 4) Checkout sÃ¼recini basitleÅŸtirin",
            "keywords": ["site", "yavaÅŸ", "hata", "bulunamadÄ±", "mobil", "arama"]
        }
    ]
    
    for entry in knowledge_entries:
        rag.add_knowledge(
            entry["category"],
            entry["problem"],
            entry["solution"],
            entry["keywords"]
        )
    
    print("âœ… Demo bilgi tabanÄ± kuruldu")


def main():
    """Demo kullanÄ±mÄ±"""
    print("ğŸš€ FAISS RAG SÄ°STEMÄ° DEMO")
    print("=" * 60)
    
    # RAG sistemi oluÅŸtur
    rag = FaissRAGSystem()
    
    # CSV'den yorumlarÄ± yÃ¼kle
    doc_count = rag.load_comments_from_csv("trendyol_comments.csv")
    if doc_count > 0:
        print(f"âœ… {doc_count} yeni yorum yÃ¼klendi")
    
    # Demo bilgi tabanÄ±nÄ± kur
    setup_demo_knowledge(rag)
    
    # Sistem istatistikleri
    stats = rag.get_stats()
    print(f"\nğŸ“Š SÄ°STEM Ä°STATÄ°STÄ°KLERÄ°:")
    print(f"   ğŸ“ Toplam Yorum: {stats['total_comments']}")
    print(f"   ğŸ“š Bilgi TabanÄ±: {stats['total_knowledge_entries']}")
    print(f"   ğŸ”¢ Vector Yorum: {stats['vector_comments']}")
    print(f"   ğŸ§  Embedding Model: {stats['embedding_model']}")
    print(f"   ğŸ“ Embedding Boyut: {stats['embedding_dimension']}")
    
    # Demo sorgularÄ±
    demo_questions = [
        "Kargo sorunlarÄ± hakkÄ±nda ÅŸikayetler var mÄ±?",
        "ÃœrÃ¼n kalitesi ile ilgili en bÃ¼yÃ¼k problem nedir?",
        "Beden uyumsuzluÄŸu nasÄ±l Ã§Ã¶zÃ¼lebilir?",
        "MÃ¼ÅŸteri hizmetleri nasÄ±l geliÅŸtirilebilir?",
        "Website performansÄ± hakkÄ±nda ne dÃ¼ÅŸÃ¼nÃ¼lÃ¼yor?"
    ]
    
    print(f"\nğŸ’¬ DEMO SORGULARI:")
    print("=" * 60)
    
    for question in demo_questions:
        print(f"\nâ“ SORU: {question}")
        print("-" * 50)
        
        result = rag.query(question)
        print(f"ğŸ¤– YANIT:\n{result['answer']}")
        print(f"ğŸ“Š Benzer yorum: {len(result['similar_comments'])}")
        print(f"ğŸ“š Bilgi sonucu: {len(result['knowledge_results'])}")
    
    print(f"\nâœ… DEMO TAMAMLANDI!")
    print("ğŸ’¡ Web arayÃ¼zÃ¼ iÃ§in: streamlit run faiss_rag_streamlit.py")


if __name__ == "__main__":
    main() 